# Gerador de Data-Schemas FISCA

Scripts para gerar automaticamente os arquivos de schema (`DESCRIBE FORMATTED` e `SELECT * LIMIT 10`) para todas as tabelas do projeto FISCA.

## üìã Tabelas que ser√£o processadas

### Tabelas Originais (5)
Tabelas persistentes no banco de dados `teste`:

1. `fisca_fiscalizacoes_consolidadas` - Tabela principal consolidada
2. `fisca_dashboard_executivo` - Dashboard executivo
3. `fisca_scores_efetividade` - Scores de efetividade
4. `fisca_metricas_por_afre` - M√©tricas por auditor fiscal
5. `fisca_acompanhamentos` - Dados de acompanhamentos

### Tabelas Intermedi√°rias (10)
DataFrames Pandas criados dinamicamente (in-memory):

1. `metrics_df` - KPIs do dashboard
2. `temporal_df` - An√°lise temporal
3. `gerencia_df` - Performance por ger√™ncia
4. `afre_df` - Performance por auditor
5. `geo_df` - Distribui√ß√£o geogr√°fica
6. `cnae_df` - An√°lise setorial
7. `ml_df` - Dataset ML
8. `network_df` - An√°lise de redes
9. `infraction_types_df` - Tipos de infra√ß√µes
10. `company_details_df` - Detalhes de empresas

## üöÄ Como Usar

### Op√ß√£o 1: Jupyter Notebook (Recomendado)

```bash
# 1. Abra o notebook no ambiente Jupyter/Zeppelin
scripts/generate_data_schemas.ipynb

# 2. Execute todas as c√©lulas
# O script vai:
# - Conectar ao banco via SparkSession
# - Executar queries para cada tabela
# - Salvar resultados em data-schemas/
```

### Op√ß√£o 2: Script Python

```bash
# No ambiente com PySpark configurado:
cd /home/user/Fisca
python scripts/generate_data_schemas.py
```

**Nota:** O script Python requer que `session.sparkSession` esteja dispon√≠vel (ambiente notebook).

### Op√ß√£o 3: Execu√ß√£o Standalone

Se quiser executar fora do notebook, edite o arquivo `generate_data_schemas.py` e descomente a se√ß√£o:

```python
# ============================================================================
# C√ìDIGO ALTERNATIVO: Para executar fora do ambiente notebook
# ============================================================================
```

## üìÅ Arquivos Gerados

Ap√≥s a execu√ß√£o, ser√° criado o diret√≥rio `data-schemas/` contendo:

### Para cada tabela original:
```
{nome_tabela}_describe_formatted.txt    # Schema completo da tabela
{nome_tabela}_select_limit_10.txt       # Primeiras 10 linhas (formato texto)
{nome_tabela}_sample_data.csv           # Primeiras 10 linhas (formato CSV)
```

### Documenta√ß√£o adicional:
```
intermediate_tables_README.txt          # Documenta√ß√£o das tabelas intermedi√°rias
SUMMARY_REPORT.txt                      # Relat√≥rio resumo da execu√ß√£o
```

## üìä Exemplo de Sa√≠da

```
data-schemas/
‚îú‚îÄ‚îÄ fisca_fiscalizacoes_consolidadas_describe_formatted.txt
‚îú‚îÄ‚îÄ fisca_fiscalizacoes_consolidadas_select_limit_10.txt
‚îú‚îÄ‚îÄ fisca_fiscalizacoes_consolidadas_sample_data.csv
‚îú‚îÄ‚îÄ fisca_dashboard_executivo_describe_formatted.txt
‚îú‚îÄ‚îÄ fisca_dashboard_executivo_select_limit_10.txt
‚îú‚îÄ‚îÄ fisca_dashboard_executivo_sample_data.csv
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ intermediate_tables_README.txt
‚îî‚îÄ‚îÄ SUMMARY_REPORT.txt
```

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis configur√°veis:

```python
DATABASE = "teste"              # Nome do database no Impala
OUTPUT_DIR = "data-schemas"     # Diret√≥rio de sa√≠da
```

### Para adicionar novas tabelas:

Edite a lista `ORIGINAL_TABLES` no script:

```python
ORIGINAL_TABLES = [
    "fisca_fiscalizacoes_consolidadas",
    "sua_nova_tabela",  # Adicione aqui
]
```

## üîß Requisitos

- Python 3.7+
- PySpark / SparkSession configurado
- Acesso ao banco de dados Impala (cluster configurado)
- Bibliotecas: `pyspark`, `pandas`

## üìù Notas Importantes

1. **Permiss√µes**: Certifique-se de ter permiss√µes de leitura nas tabelas
2. **Performance**: Cada tabela executa 2 queries (DESCRIBE + SELECT)
3. **Rede**: Requer conex√£o com o cluster Impala
4. **Tamanho**: Os arquivos gerados podem ocupar espa√ßo significativo

## ‚ùì Troubleshooting

### Erro: "SparkSession n√£o encontrada"
```
‚ùå ERRO: SparkSession n√£o encontrada!
```

**Solu√ß√£o**: Execute o script dentro de um notebook Jupyter/Zeppelin com Spark configurado.

### Erro: "Table not found"
```
‚ùå Erro ao processar tabela_xyz: Table not found
```

**Solu√ß√£o**: Verifique se a tabela existe no database `teste`:
```python
spark.sql("SHOW TABLES IN teste").show()
```

### Erro de permiss√µes
```
‚ùå Access denied for user...
```

**Solu√ß√£o**: Verifique suas credenciais e permiss√µes no cluster.

## üìû Suporte

Para d√∫vidas ou problemas:
1. Verifique os logs gerados em `data-schemas/`
2. Consulte o arquivo `SUMMARY_REPORT.txt`
3. Arquivos de erro: `{tabela}_ERROR.txt`

## üéØ Pr√≥ximos Passos

Ap√≥s gerar os schemas:

1. Revise os arquivos em `data-schemas/`
2. Valide a estrutura das tabelas
3. Use os schemas para documentar o projeto
4. Commit os arquivos no reposit√≥rio (se necess√°rio)

---

**√öltima atualiza√ß√£o:** 2025-11-17
**Vers√£o:** 1.0.0
