{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_fisca\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c00480-e513-4e68-a3ad-f199dea3987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 1: SETUP E IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_curve, auc, silhouette_score,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# PySpark imports com aliases\n",
    "from pyspark.sql.functions import (\n",
    "    col as spark_col,\n",
    "    sum as spark_sum,\n",
    "    avg as spark_avg,\n",
    "    count as spark_count,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    when as spark_when,\n",
    "    desc as spark_desc,\n",
    "    asc as spark_asc,\n",
    "    round as spark_round,\n",
    "    coalesce as spark_coalesce\n",
    ")\n",
    "\n",
    "# Acesso ao SparkSession\n",
    "spark = session.sparkSession\n",
    "\n",
    "# Configura√ß√µes visuais\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports realizados com sucesso!\")\n",
    "print(f\"üì¶ Pandas: {pd.__version__}\")\n",
    "print(f\"üì¶ NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c787d15-aee0-4615-886a-fc4790989835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 2: FUN√á√ïES AUXILIARES\n",
    "# ============================================================================\n",
    "\n",
    "def load_spark_to_pandas_safe(query, limit=100000, view_name=\"temp_view\"):\n",
    "    \"\"\"\n",
    "    Carrega dados do Spark para Pandas de forma segura\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar view tempor√°ria\n",
    "        spark.sql(f\"CREATE OR REPLACE TEMPORARY VIEW {view_name} AS {query}\")\n",
    "        \n",
    "        # Verificar tamanho\n",
    "        total = spark.sql(f\"SELECT COUNT(*) as total FROM {view_name}\").collect()[0]['total']\n",
    "        print(f\"üìä Total de registros encontrados: {total:,}\")\n",
    "        \n",
    "        if total == 0:\n",
    "            print(\"‚ö†Ô∏è Nenhum registro encontrado!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        elif total > limit:\n",
    "            print(f\"‚ö†Ô∏è Muitos registros ({total:,}), limitando a {limit:,}...\")\n",
    "            df_pandas = spark.sql(f\"SELECT * FROM {view_name} LIMIT {limit}\").toPandas()\n",
    "        \n",
    "        else:\n",
    "            df_spark = spark.sql(f\"SELECT * FROM {view_name}\")\n",
    "            df_spark.cache()\n",
    "            df_pandas = df_spark.toPandas()\n",
    "            df_spark.unpersist()\n",
    "        \n",
    "        print(f\"‚úÖ DataFrame carregado: {df_pandas.shape[0]:,} linhas x {df_pandas.shape[1]} colunas\")\n",
    "        return df_pandas\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar dados: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def format_currency(value):\n",
    "    \"\"\"Formata valor em moeda brasileira\"\"\"\n",
    "    return f\"R$ {value:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n",
    "\n",
    "\n",
    "def format_percentage(value):\n",
    "    \"\"\"Formata percentual\"\"\"\n",
    "    return f\"{value:.2f}%\"\n",
    "\n",
    "\n",
    "def create_metric_card(value, title, suffix=\"\", prefix=\"\"):\n",
    "    \"\"\"Cria card de m√©trica para visualiza√ß√£o\"\"\"\n",
    "    return f\"\"\"\n",
    "    <div style='text-align: center; padding: 20px; background-color: #f0f2f6; border-radius: 10px;'>\n",
    "        <h3 style='color: #0e1117; margin: 0;'>{title}</h3>\n",
    "        <h1 style='color: #ff4b4b; margin: 10px 0;'>{prefix}{value:,.2f}{suffix}</h1>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes auxiliares carregadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468cd16-7b80-42cb-8655-24d62bb264f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 3: CARREGAMENTO DASHBOARD EXECUTIVO\n",
    "# ============================================================================\n",
    "\n",
    "query_dashboard = \"\"\"\n",
    "SELECT \n",
    "    ano,\n",
    "    CAST(COALESCE(qtd_acompanhamentos, 0) AS DOUBLE) AS qtd_acompanhamentos,\n",
    "    CAST(COALESCE(empresas_acompanhadas, 0) AS DOUBLE) AS empresas_acompanhadas,\n",
    "    CAST(COALESCE(qtd_infracoes_lavradas, 0) AS DOUBLE) AS qtd_infracoes_lavradas,\n",
    "    CAST(COALESCE(empresas_fiscalizadas, 0) AS DOUBLE) AS empresas_fiscalizadas,\n",
    "    CAST(COALESCE(infracoes_com_ciencia, 0) AS DOUBLE) AS infracoes_com_ciencia,\n",
    "    CAST(COALESCE(valor_total_infracoes, 0) AS DOUBLE) AS valor_total_infracoes,\n",
    "    CAST(COALESCE(valor_imposto_infracoes, 0) AS DOUBLE) AS valor_imposto_infracoes,\n",
    "    CAST(COALESCE(valor_multa_infracoes, 0) AS DOUBLE) AS valor_multa_infracoes,\n",
    "    CAST(COALESCE(valor_juros_infracoes, 0) AS DOUBLE) AS valor_juros_infracoes,\n",
    "    CAST(COALESCE(qtd_nfs_emitidas, 0) AS DOUBLE) AS qtd_nfs_emitidas,\n",
    "    CAST(COALESCE(valor_total_nfs, 0) AS DOUBLE) AS valor_total_nfs,\n",
    "    CAST(COALESCE(valor_imposto_nfs, 0) AS DOUBLE) AS valor_imposto_nfs,\n",
    "    CAST(COALESCE(valor_multa_nfs, 0) AS DOUBLE) AS valor_multa_nfs,\n",
    "    CAST(COALESCE(valor_juros_nfs, 0) AS DOUBLE) AS valor_juros_nfs,\n",
    "    CAST(COALESCE(qtd_encerramentos, 0) AS DOUBLE) AS qtd_encerramentos,\n",
    "    CAST(COALESCE(qtd_encerramentos_com_resultado, 0) AS DOUBLE) AS qtd_encerramentos_com_resultado,\n",
    "    CAST(COALESCE(qtd_ciclos_completos, 0) AS DOUBLE) AS qtd_ciclos_completos,\n",
    "    CAST(COALESCE(media_dias_infracao_nf, 0) AS DOUBLE) AS media_dias_infracao_nf,\n",
    "    CAST(COALESCE(media_dias_infracao_encerramento, 0) AS DOUBLE) AS media_dias_infracao_encerramento,\n",
    "    CAST(COALESCE(taxa_conversao_infracao_nf, 0) AS DOUBLE) AS taxa_conversao_infracao_nf,\n",
    "    CAST(COALESCE(valor_medio_infracao, 0) AS DOUBLE) AS valor_medio_infracao,\n",
    "    CAST(COALESCE(valor_medio_nf, 0) AS DOUBLE) AS valor_medio_nf,\n",
    "    CAST(COALESCE(qtd_afres_ativos, 0) AS DOUBLE) AS qtd_afres_ativos,\n",
    "    CAST(COALESCE(media_infracoes_por_afre, 0) AS DOUBLE) AS media_infracoes_por_afre,\n",
    "    CAST(COALESCE(valor_medio_por_afre, 0) AS DOUBLE) AS valor_medio_por_afre\n",
    "FROM teste.fisca_dashboard_executivo\n",
    "ORDER BY ano DESC\n",
    "\"\"\"\n",
    "\n",
    "df_dashboard = load_spark_to_pandas_safe(query_dashboard, view_name=\"vw_dashboard\")\n",
    "\n",
    "# Exibir resumo\n",
    "if not df_dashboard.empty:\n",
    "    print(\"\\nüìä RESUMO DO DASHBOARD EXECUTIVO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Per√≠odo: {df_dashboard['ano'].min()} a {df_dashboard['ano'].max()}\")\n",
    "    print(f\"Total de Infra√ß√µes: {df_dashboard['qtd_infracoes_lavradas'].sum():,.0f}\")\n",
    "    print(f\"Total de NFs: {df_dashboard['qtd_nfs_emitidas'].sum():,.0f}\")\n",
    "    print(f\"Valor Total Infra√ß√µes: {format_currency(df_dashboard['valor_total_infracoes'].sum())}\")\n",
    "    print(f\"Valor Total NFs: {format_currency(df_dashboard['valor_total_nfs'].sum())}\")\n",
    "    print(f\"Taxa Convers√£o M√©dia: {df_dashboard['taxa_conversao_infracao_nf'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492cde1-2190-47b0-88bc-5e5c442fae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 4: EDA - AN√ÅLISE EXPLORAT√ìRIA DASHBOARD EXECUTIVO\n",
    "# ============================================================================\n",
    "\n",
    "if not df_dashboard.empty:\n",
    "    \n",
    "    print(\"üìä ESTAT√çSTICAS DESCRITIVAS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Estat√≠sticas principais\n",
    "    stats_cols = [\n",
    "        'qtd_infracoes_lavradas', 'qtd_nfs_emitidas', 'valor_total_infracoes',\n",
    "        'valor_total_nfs', 'taxa_conversao_infracao_nf', 'media_dias_infracao_nf'\n",
    "    ]\n",
    "    \n",
    "    for col in stats_cols:\n",
    "        if col in df_dashboard.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  M√©dia: {df_dashboard[col].mean():,.2f}\")\n",
    "            print(f\"  Mediana: {df_dashboard[col].median():,.2f}\")\n",
    "            print(f\"  Desvio Padr√£o: {df_dashboard[col].std():,.2f}\")\n",
    "            print(f\"  Min: {df_dashboard[col].min():,.2f}\")\n",
    "            print(f\"  Max: {df_dashboard[col].max():,.2f}\")\n",
    "    \n",
    "    # Verificar valores nulos\n",
    "    print(\"\\n\\nüìã VALORES NULOS POR COLUNA\")\n",
    "    print(\"=\" * 80)\n",
    "    null_counts = df_dashboard.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        print(null_counts[null_counts > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ Nenhum valor nulo encontrado!\")\n",
    "    \n",
    "    # Correla√ß√µes\n",
    "    print(\"\\n\\nüîó CORRELA√á√ïES PRINCIPAIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    numeric_cols = df_dashboard.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = df_dashboard[numeric_cols].corr()\n",
    "    \n",
    "    # Top 10 correla√ß√µes\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_pairs.append({\n",
    "                'var1': corr_matrix.columns[i],\n",
    "                'var2': corr_matrix.columns[j],\n",
    "                'correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "    \n",
    "    corr_df = pd.DataFrame(corr_pairs).sort_values('correlation', ascending=False)\n",
    "    print(\"\\nTop 10 Correla√ß√µes Positivas:\")\n",
    "    print(corr_df.head(10))\n",
    "    \n",
    "    print(\"\\nTop 10 Correla√ß√µes Negativas:\")\n",
    "    print(corr_df.tail(10))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame vazio - pulando an√°lise explorat√≥ria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0acb25-ba15-4859-bbbd-d72fd090cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 5: VISUALIZA√á√ÉO - EVOLU√á√ÉO TEMPORAL\n",
    "# ============================================================================\n",
    "\n",
    "if not df_dashboard.empty:\n",
    "    \n",
    "    # Criar figura com subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Evolu√ß√£o de Infra√ß√µes e NFs',\n",
    "            'Evolu√ß√£o dos Valores (R$ Milh√µes)',\n",
    "            'Taxa de Convers√£o ao Longo do Tempo',\n",
    "            'Produtividade por AFRE'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Infra√ß√µes e NFs\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_dashboard['ano'],\n",
    "            y=df_dashboard['qtd_infracoes_lavradas'],\n",
    "            name='Infra√ß√µes Lavradas',\n",
    "            marker_color='indianred'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_dashboard['ano'],\n",
    "            y=df_dashboard['qtd_nfs_emitidas'],\n",
    "            name='NFs Emitidas',\n",
    "            marker_color='lightsalmon'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Valores\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_dashboard['ano'],\n",
    "            y=df_dashboard['valor_total_infracoes'] / 1_000_000,\n",
    "            name='Valor Infra√ß√µes',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='blue', width=3)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_dashboard['ano'],\n",
    "            y=df_dashboard['valor_total_nfs'] / 1_000_000,\n",
    "            name='Valor NFs',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='green', width=3)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Taxa de Convers√£o\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_dashboard['ano'],\n",
    "            y=df_dashboard['taxa_conversao_infracao_nf'],\n",
    "            name='Taxa Convers√£o',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='purple', width=3),\n",
    "            fill='tozeroy'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Produtividade\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_dashboard['ano'],\n",
    "            y=df_dashboard['media_infracoes_por_afre'],\n",
    "            name='Infra√ß√µes/AFRE',\n",
    "            marker_color='teal'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        title_text=\"üìä Dashboard Executivo - Evolu√ß√£o Temporal FISCA\",\n",
    "        title_font_size=20\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Ano\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Ano\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Quantidade\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"R$ Milh√µes\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"%\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Infra√ß√µes/AFRE\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°fico de evolu√ß√£o temporal criado!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame vazio - pulando visualiza√ß√£o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2bb1c-9c55-4d84-84a2-9d0beb3c0ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 6: CARREGAMENTO FISCALIZA√á√ïES CONSOLIDADAS\n",
    "# ============================================================================\n",
    "\n",
    "query_fiscalizacoes = \"\"\"\n",
    "SELECT \n",
    "    cnpj,\n",
    "    nm_razao_social,\n",
    "    regime_tributario,\n",
    "    municipio,\n",
    "    uf,\n",
    "    cnae_secao,\n",
    "    cnae_secao_descricao,\n",
    "    ano_infracao,\n",
    "    CAST(COALESCE(valor_total_infracao, 0) AS DOUBLE) AS valor_total_infracao,\n",
    "    CAST(COALESCE(valor_imposto_infracao, 0) AS DOUBLE) AS valor_imposto_infracao,\n",
    "    CAST(COALESCE(valor_multa_infracao, 0) AS DOUBLE) AS valor_multa_infracao,\n",
    "    CAST(COALESCE(valor_total_nf, 0) AS DOUBLE) AS valor_total_nf,\n",
    "    CAST(COALESCE(gerou_notificacao, 0) AS INT) AS gerou_notificacao,\n",
    "    CAST(COALESCE(teve_encerramento, 0) AS INT) AS teve_encerramento,\n",
    "    CAST(COALESCE(ciclo_completo, 0) AS INT) AS ciclo_completo,\n",
    "    CAST(COALESCE(dias_infracao_ate_nf, 0) AS DOUBLE) AS dias_infracao_ate_nf,\n",
    "    CAST(COALESCE(pagou_dp_infracao, 0) AS INT) AS pagou_dp_infracao,\n",
    "    situacao_final\n",
    "FROM teste.fisca_fiscalizacoes_consolidadas\n",
    "WHERE ano_infracao >= 2020\n",
    "\"\"\"\n",
    "\n",
    "df_fisc = load_spark_to_pandas_safe(\n",
    "    query_fiscalizacoes, \n",
    "    limit=150000,\n",
    "    view_name=\"vw_fiscalizacoes\"\n",
    ")\n",
    "\n",
    "if not df_fisc.empty:\n",
    "    print(\"\\nüìä RESUMO FISCALIZA√á√ïES CONSOLIDADAS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total de registros: {len(df_fisc):,}\")\n",
    "    print(f\"Empresas √∫nicas: {df_fisc['cnpj'].nunique():,}\")\n",
    "    print(f\"Valor total infra√ß√µes: {format_currency(df_fisc['valor_total_infracao'].sum())}\")\n",
    "    print(f\"Valor total NFs: {format_currency(df_fisc['valor_total_nf'].sum())}\")\n",
    "    print(f\"\\nDistribui√ß√£o por Situa√ß√£o:\")\n",
    "    print(df_fisc['situacao_final'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2abc1-1ad5-4b39-a830-41d3aaabd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 7: EDA - AN√ÅLISE POR SETOR (CNAE)\n",
    "# ============================================================================\n",
    "\n",
    "if not df_fisc.empty:\n",
    "    \n",
    "    print(\"üìä AN√ÅLISE POR SETOR ECON√îMICO (CNAE)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Agrupar por se√ß√£o CNAE\n",
    "    setor_analysis = df_fisc.groupby('cnae_secao_descricao').agg({\n",
    "        'cnpj': 'count',\n",
    "        'valor_total_infracao': 'sum',\n",
    "        'valor_total_nf': 'sum',\n",
    "        'gerou_notificacao': 'sum',\n",
    "        'ciclo_completo': 'sum',\n",
    "        'dias_infracao_ate_nf': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    setor_analysis.columns = [\n",
    "        'setor', 'qtd_fiscalizacoes', 'valor_infracoes', \n",
    "        'valor_nfs', 'qtd_nfs', 'ciclos_completos', 'media_dias'\n",
    "    ]\n",
    "    \n",
    "    # Calcular taxa de convers√£o\n",
    "    setor_analysis['taxa_conversao'] = (\n",
    "        setor_analysis['qtd_nfs'] / setor_analysis['qtd_fiscalizacoes'] * 100\n",
    "    )\n",
    "    \n",
    "    # Ordenar por valor\n",
    "    setor_analysis = setor_analysis.sort_values('valor_nfs', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Setores por Valor de NFs:\")\n",
    "    print(setor_analysis.head(10)[['setor', 'qtd_fiscalizacoes', 'valor_nfs', 'taxa_conversao']])\n",
    "    \n",
    "    # Visualiza√ß√£o - Top 15 setores\n",
    "    top_setores = setor_analysis.head(15)\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('Valor de NFs por Setor (Top 15)', 'Taxa de Convers√£o por Setor (Top 15)'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # Gr√°fico 1: Valores\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=top_setores['setor'],\n",
    "            x=top_setores['valor_nfs'] / 1_000_000,\n",
    "            orientation='h',\n",
    "            marker_color='steelblue',\n",
    "            name='Valor NFs'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Gr√°fico 2: Taxa de convers√£o\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=top_setores['setor'],\n",
    "            x=top_setores['taxa_conversao'],\n",
    "            orientation='h',\n",
    "            marker_color='coral',\n",
    "            name='Taxa Convers√£o'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        title_text=\"üìä An√°lise por Setor Econ√¥mico\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"R$ Milh√µes\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"% Convers√£o\", row=1, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise por setor conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame vazio - pulando an√°lise por setor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb716d-9c2e-4d39-9a73-3edd1d751384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 8: AN√ÅLISE GEOGR√ÅFICA (CORRIGIDA)\n",
    "# ============================================================================\n",
    "\n",
    "if not df_fisc.empty:\n",
    "    \n",
    "    print(\"üó∫Ô∏è AN√ÅLISE GEOGR√ÅFICA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Agrupar por munic√≠pio (SEM taxa_conversao no agg)\n",
    "    geo_analysis = df_fisc.groupby(['municipio', 'uf']).agg({\n",
    "        'cnpj': 'count',\n",
    "        'valor_total_infracao': 'sum',\n",
    "        'valor_total_nf': 'sum',\n",
    "        'gerou_notificacao': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    geo_analysis.columns = [\n",
    "        'municipio', 'uf', 'qtd_fiscalizacoes', \n",
    "        'valor_infracoes', 'valor_nfs', 'qtd_nfs'\n",
    "    ]\n",
    "    \n",
    "    # Calcular taxa de convers√£o DEPOIS do reset_index\n",
    "    geo_analysis['taxa_conversao'] = (\n",
    "        geo_analysis['qtd_nfs'] / geo_analysis['qtd_fiscalizacoes'] * 100\n",
    "    )\n",
    "    \n",
    "    # Top 20 munic√≠pios\n",
    "    top_municipios = geo_analysis.nlargest(20, 'valor_nfs')\n",
    "    \n",
    "    print(\"\\nTop 20 Munic√≠pios por Valor de NFs:\")\n",
    "    print(top_municipios[['municipio', 'uf', 'qtd_fiscalizacoes', 'valor_nfs']])\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    fig = px.bar(\n",
    "        top_municipios,\n",
    "        x='valor_nfs',\n",
    "        y='municipio',\n",
    "        color='uf',\n",
    "        orientation='h',\n",
    "        title='Top 20 Munic√≠pios por Valor de NFs Emitidas',\n",
    "        labels={'valor_nfs': 'Valor Total NFs (R$)', 'municipio': 'Munic√≠pio'},\n",
    "        text='qtd_fiscalizacoes'\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(texttemplate='%{text} fisc.', textposition='outside')\n",
    "    fig.update_layout(height=700, showlegend=True)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Mapa de calor - Distribui√ß√£o por UF\n",
    "    uf_analysis = df_fisc.groupby('uf').agg({\n",
    "        'cnpj': 'count',\n",
    "        'valor_total_nf': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig2 = go.Figure(data=go.Choropleth(\n",
    "        locations=uf_analysis['uf'],\n",
    "        z=uf_analysis['valor_total_nf'],\n",
    "        locationmode='USA-states',\n",
    "        colorscale='Reds',\n",
    "        text=uf_analysis['uf'],\n",
    "        colorbar_title=\"Valor NFs\"\n",
    "    ))\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title_text='Distribui√ß√£o Geogr√°fica - Valor de NFs por UF',\n",
    "        geo_scope='south america',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig2.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise geogr√°fica conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame vazio - pulando an√°lise geogr√°fica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992bc1f-f50b-488f-9d78-5ab96fc31c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 9: PREPARA√á√ÉO PARA MACHINE LEARNING\n",
    "# ============================================================================\n",
    "\n",
    "if not df_fisc.empty and len(df_fisc) >= 100:\n",
    "    \n",
    "    print(\"ü§ñ PREPARA√á√ÉO DOS DADOS PARA MACHINE LEARNING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Criar dataset para ML\n",
    "    df_ml = df_fisc.copy()\n",
    "    \n",
    "    # Remover valores nulos e criar features\n",
    "    df_ml = df_ml.dropna(subset=[\n",
    "        'valor_total_infracao', 'regime_tributario', \n",
    "        'cnae_secao', 'municipio'\n",
    "    ])\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df_ml['valor_log'] = np.log1p(df_ml['valor_total_infracao'])\n",
    "    df_ml['tem_multa'] = (df_ml['valor_multa_infracao'] > 0).astype(int)\n",
    "    df_ml['perc_multa'] = np.where(\n",
    "        df_ml['valor_total_infracao'] > 0,\n",
    "        df_ml['valor_multa_infracao'] / df_ml['valor_total_infracao'] * 100,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Faixa de valor\n",
    "    df_ml['faixa_valor'] = pd.cut(\n",
    "        df_ml['valor_total_infracao'],\n",
    "        bins=[0, 50000, 100000, 500000, 1000000, np.inf],\n",
    "        labels=['Muito Baixo', 'Baixo', 'M√©dio', 'Alto', 'Muito Alto']\n",
    "    )\n",
    "    \n",
    "    # Encoding de vari√°veis categ√≥ricas\n",
    "    le_regime = LabelEncoder()\n",
    "    le_cnae = LabelEncoder()\n",
    "    le_municipio = LabelEncoder()\n",
    "    le_uf = LabelEncoder()\n",
    "    \n",
    "    df_ml['regime_encoded'] = le_regime.fit_transform(df_ml['regime_tributario'].fillna('DESCONHECIDO'))\n",
    "    df_ml['cnae_encoded'] = le_cnae.fit_transform(df_ml['cnae_secao'].fillna('DESCONHECIDO'))\n",
    "    df_ml['municipio_encoded'] = le_municipio.fit_transform(df_ml['municipio'].fillna('DESCONHECIDO'))\n",
    "    df_ml['uf_encoded'] = le_uf.fit_transform(df_ml['uf'].fillna('SC'))\n",
    "    \n",
    "    # Selecionar features para modelo\n",
    "    feature_cols = [\n",
    "        'valor_log', 'valor_imposto_infracao', 'valor_multa_infracao',\n",
    "        'tem_multa', 'perc_multa', 'regime_encoded', 'cnae_encoded',\n",
    "        'municipio_encoded', 'uf_encoded', 'ano_infracao',\n",
    "        'pagou_dp_infracao'\n",
    "    ]\n",
    "    \n",
    "    # Target: gerou_notificacao\n",
    "    X = df_ml[feature_cols].fillna(0)\n",
    "    y = df_ml['gerou_notificacao']\n",
    "    \n",
    "    print(f\"\\nüìä Dataset ML preparado:\")\n",
    "    print(f\"  - Features: {X.shape[1]}\")\n",
    "    print(f\"  - Amostras: {X.shape[0]:,}\")\n",
    "    print(f\"  - Target positivo: {y.sum():,} ({y.mean()*100:.2f}%)\")\n",
    "    print(f\"  - Target negativo: {(~y.astype(bool)).sum():,} ({(1-y.mean())*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dados preparados para Machine Learning!\")\n",
    "    \n",
    "    # Salvar para pr√≥ximas c√©lulas\n",
    "    ml_data = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'df_ml': df_ml,\n",
    "        'feature_cols': feature_cols,\n",
    "        'encoders': {\n",
    "            'regime': le_regime,\n",
    "            'cnae': le_cnae,\n",
    "            'municipio': le_municipio,\n",
    "            'uf': le_uf\n",
    "        }\n",
    "    }\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados insuficientes para Machine Learning\")\n",
    "    ml_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cbb195-ed4c-4416-afa3-f0d185abaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 10: RANDOM FOREST - PREVIS√ÉO DE NOTIFICA√á√ÉO\n",
    "# ============================================================================\n",
    "\n",
    "if ml_data is not None:\n",
    "    \n",
    "    print(\"üå≤ RANDOM FOREST - PREVIS√ÉO DE NOTIFICA√á√ÉO FISCAL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    X = ml_data['X']\n",
    "    y = ml_data['y']\n",
    "    \n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Distribui√ß√£o dos dados:\")\n",
    "    print(f\"  Treino: {len(X_train):,} amostras\")\n",
    "    print(f\"  Teste: {len(X_test):,} amostras\")\n",
    "    \n",
    "    # Normaliza√ß√£o\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Treinar Random Forest\n",
    "    print(\"\\nüîÑ Treinando Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred = rf_model.predict(X_test_scaled)\n",
    "    y_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    print(\"\\nüìä RESULTADOS DO MODELO:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(classification_report(y_test, y_pred, target_names=['N√£o Notificou', 'Notificou']))\n",
    "    \n",
    "    # Import√¢ncia das features\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': ml_data['feature_cols'],\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüîù Top 10 Features Mais Importantes:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Visualiza√ß√£o - Feature Importance\n",
    "    fig = px.bar(\n",
    "        feature_importance.head(15),\n",
    "        x='importance',\n",
    "        y='feature',\n",
    "        orientation='h',\n",
    "        title='Import√¢ncia das Features - Random Forest',\n",
    "        labels={'importance': 'Import√¢ncia', 'feature': 'Feature'}\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig_roc = go.Figure()\n",
    "    fig_roc.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'ROC (AUC = {roc_auc:.3f})',\n",
    "        line=dict(color='darkorange', width=2)\n",
    "    ))\n",
    "    fig_roc.add_trace(go.Scatter(\n",
    "        x=[0, 1], y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Random',\n",
    "        line=dict(color='navy', width=2, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig_roc.update_layout(\n",
    "        title='Curva ROC - Random Forest',\n",
    "        xaxis_title='Taxa de Falsos Positivos',\n",
    "        yaxis_title='Taxa de Verdadeiros Positivos',\n",
    "        height=500\n",
    "    )\n",
    "    fig_roc.show()\n",
    "    \n",
    "    # Matriz de Confus√£o\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig_cm = px.imshow(\n",
    "        cm,\n",
    "        text_auto=True,\n",
    "        labels=dict(x=\"Predito\", y=\"Real\", color=\"Quantidade\"),\n",
    "        x=['N√£o Notificou', 'Notificou'],\n",
    "        y=['N√£o Notificou', 'Notificou'],\n",
    "        title='Matriz de Confus√£o - Random Forest'\n",
    "    )\n",
    "    fig_cm.show()\n",
    "    \n",
    "    # Salvar modelo\n",
    "    rf_results = {\n",
    "        'model': rf_model,\n",
    "        'scaler': scaler,\n",
    "        'feature_importance': feature_importance,\n",
    "        'auc': roc_auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(\"\\n‚úÖ Random Forest treinado com sucesso!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados ML n√£o dispon√≠veis\")\n",
    "    rf_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3982218-30ca-435a-90bb-ce5bd8616a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 11: XGBOOST - PREVIS√ÉO DE NOTIFICA√á√ÉO FISCAL (CORRIGIDA)\n",
    "# ============================================================================\n",
    "\n",
    "if ml_data is not None:\n",
    "    \n",
    "    print(\"üöÄ XGBOOST - PREVIS√ÉO DE NOTIFICA√á√ÉO FISCAL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    X = ml_data['X']\n",
    "    y = ml_data['y']\n",
    "    \n",
    "    # Split train/test (mesmo split do RF para compara√ß√£o)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Calcular scale_pos_weight para balanceamento\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    print(f\"\\nüìä Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Treinar XGBoost\n",
    "    print(\"\\nüîÑ Treinando XGBoost...\")\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predi√ß√µes\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # M√©tricas\n",
    "    print(\"\\nüìä RESULTADOS DO MODELO XGBOOST:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(classification_report(y_test, y_pred_xgb, target_names=['N√£o Notificou', 'Notificou']))\n",
    "    \n",
    "    # Import√¢ncia das features\n",
    "    feature_importance_xgb = pd.DataFrame({\n",
    "        'feature': ml_data['feature_cols'],\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüîù Top 10 Features Mais Importantes (XGBoost):\")\n",
    "    print(feature_importance_xgb.head(10))\n",
    "    \n",
    "    # Visualiza√ß√£o - Feature Importance\n",
    "    fig = px.bar(\n",
    "        feature_importance_xgb.head(15),\n",
    "        x='importance',\n",
    "        y='feature',\n",
    "        orientation='h',\n",
    "        title='Import√¢ncia das Features - XGBoost',\n",
    "        labels={'importance': 'Import√¢ncia', 'feature': 'Feature'},\n",
    "        color='importance',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "    roc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
    "    \n",
    "    # Comparar com Random Forest\n",
    "    if rf_results is not None:\n",
    "        fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_results['probabilities'])\n",
    "        \n",
    "        fig_comp = go.Figure()\n",
    "        fig_comp.add_trace(go.Scatter(\n",
    "            x=fpr_xgb, y=tpr_xgb,\n",
    "            mode='lines',\n",
    "            name=f'XGBoost (AUC = {roc_auc_xgb:.3f})',\n",
    "            line=dict(color='green', width=2)\n",
    "        ))\n",
    "        fig_comp.add_trace(go.Scatter(\n",
    "            x=fpr_rf, y=tpr_rf,\n",
    "            mode='lines',\n",
    "            name=f'Random Forest (AUC = {rf_results[\"auc\"]:.3f})',\n",
    "            line=dict(color='darkorange', width=2)\n",
    "        ))\n",
    "        fig_comp.add_trace(go.Scatter(\n",
    "            x=[0, 1], y=[0, 1],\n",
    "            mode='lines',\n",
    "            name='Random',\n",
    "            line=dict(color='navy', width=2, dash='dash')\n",
    "        ))\n",
    "        \n",
    "        fig_comp.update_layout(\n",
    "            title='Compara√ß√£o de Modelos - Curva ROC',\n",
    "            xaxis_title='Taxa de Falsos Positivos',\n",
    "            yaxis_title='Taxa de Verdadeiros Positivos',\n",
    "            height=500\n",
    "        )\n",
    "        fig_comp.show()\n",
    "    \n",
    "    # Matriz de Confus√£o\n",
    "    cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "    fig_cm = px.imshow(\n",
    "        cm_xgb,\n",
    "        text_auto=True,\n",
    "        labels=dict(x=\"Predito\", y=\"Real\", color=\"Quantidade\"),\n",
    "        x=['N√£o Notificou', 'Notificou'],\n",
    "        y=['N√£o Notificou', 'Notificou'],\n",
    "        title='Matriz de Confus√£o - XGBoost',\n",
    "        color_continuous_scale='Blues'\n",
    "    )\n",
    "    fig_cm.show()\n",
    "    \n",
    "    # Salvar modelo\n",
    "    xgb_results = {\n",
    "        'model': xgb_model,\n",
    "        'feature_importance': feature_importance_xgb,\n",
    "        'auc': roc_auc_xgb,\n",
    "        'predictions': y_pred_xgb,\n",
    "        'probabilities': y_pred_proba_xgb\n",
    "    }\n",
    "    \n",
    "    print(\"\\n‚úÖ XGBoost treinado com sucesso!\")\n",
    "    print(f\"üéØ AUC XGBoost: {roc_auc_xgb:.4f}\")\n",
    "    if rf_results is not None:\n",
    "        print(f\"üéØ AUC Random Forest: {rf_results['auc']:.4f}\")\n",
    "        # CORRIGIDO: usar np.abs ao inv√©s de abs\n",
    "        diferenca = np.abs(roc_auc_xgb - rf_results['auc'])\n",
    "        print(f\"üìä Diferen√ßa: {diferenca:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados ML n√£o dispon√≠veis\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c6b97-97bf-4b39-b4b8-4bffada5f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 12: AN√ÅLISE DE CLUSTERS - K-MEANS (OTIMIZADA)\n",
    "# ============================================================================\n",
    "\n",
    "if ml_data is not None:\n",
    "    \n",
    "    print(\"üéØ AN√ÅLISE DE CLUSTERS - K-MEANS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_ml = ml_data['df_ml']\n",
    "    \n",
    "    # Selecionar features para clustering\n",
    "    cluster_features = [\n",
    "        'valor_log', 'valor_imposto_infracao', 'valor_multa_infracao',\n",
    "        'perc_multa', 'regime_encoded', 'cnae_encoded',\n",
    "        'dias_infracao_ate_nf', 'ano_infracao'\n",
    "    ]\n",
    "    \n",
    "    # Preparar dados\n",
    "    X_cluster = df_ml[cluster_features].fillna(0)\n",
    "    \n",
    "    # OTIMIZA√á√ÉO: Usar amostra para c√°lculo de silhouette se dataset muito grande\n",
    "    if len(X_cluster) > 10000:\n",
    "        print(f\"\\n‚ö° Dataset grande ({len(X_cluster):,} registros)\")\n",
    "        print(\"   Usando amostra de 10.000 registros para otimizar c√°lculo do cotovelo\")\n",
    "        X_cluster_sample = X_cluster.sample(n=10000, random_state=42)\n",
    "    else:\n",
    "        X_cluster_sample = X_cluster.copy()\n",
    "    \n",
    "    # Normalizar\n",
    "    scaler_cluster = StandardScaler()\n",
    "    X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "    X_cluster_sample_scaled = scaler_cluster.transform(X_cluster_sample)\n",
    "    \n",
    "    # M√©todo do Cotovelo para encontrar k ideal\n",
    "    print(\"\\nüìä Calculando n√∫mero ideal de clusters (M√©todo do Cotovelo)...\")\n",
    "    \n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, 11)\n",
    "    \n",
    "    for k in K_range:\n",
    "        print(f\"   Testando k={k}...\", end=\" \")\n",
    "        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=100)\n",
    "        kmeans_temp.fit(X_cluster_sample_scaled)\n",
    "        inertias.append(kmeans_temp.inertia_)\n",
    "        \n",
    "        # Calcular silhouette na amostra\n",
    "        sil_score = silhouette_score(X_cluster_sample_scaled, kmeans_temp.labels_, sample_size=5000)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        print(f\"Silhouette: {sil_score:.3f}\")\n",
    "    \n",
    "    # Visualizar m√©todo do cotovelo\n",
    "    fig_elbow = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('M√©todo do Cotovelo', 'Silhouette Score')\n",
    "    )\n",
    "    \n",
    "    fig_elbow.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(K_range), y=inertias,\n",
    "            mode='lines+markers',\n",
    "            name='In√©rcia',\n",
    "            line=dict(color='blue', width=2),\n",
    "            marker=dict(size=10)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig_elbow.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(K_range), y=silhouette_scores,\n",
    "            mode='lines+markers',\n",
    "            name='Silhouette',\n",
    "            line=dict(color='red', width=2),\n",
    "            marker=dict(size=10)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig_elbow.update_xaxes(title_text=\"N√∫mero de Clusters\", row=1, col=1)\n",
    "    fig_elbow.update_xaxes(title_text=\"N√∫mero de Clusters\", row=1, col=2)\n",
    "    fig_elbow.update_yaxes(title_text=\"In√©rcia\", row=1, col=1)\n",
    "    fig_elbow.update_yaxes(title_text=\"Silhouette Score\", row=1, col=2)\n",
    "    fig_elbow.update_layout(height=400, showlegend=False, title_text=\"An√°lise do N√∫mero Ideal de Clusters\")\n",
    "    \n",
    "    fig_elbow.show()\n",
    "    \n",
    "    # Escolher k ideal (maior silhouette)\n",
    "    k_ideal = K_range[np.argmax(silhouette_scores)]\n",
    "    max_silhouette = np.max(silhouette_scores)\n",
    "    print(f\"\\nüéØ N√∫mero ideal de clusters: {k_ideal}\")\n",
    "    print(f\"üìä Silhouette Score: {max_silhouette:.4f}\")\n",
    "    \n",
    "    # Treinar K-Means com k ideal NO DATASET COMPLETO\n",
    "    print(f\"\\nüîÑ Treinando K-Means com {k_ideal} clusters no dataset completo...\")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k_ideal, random_state=42, n_init=10, max_iter=300)\n",
    "    df_ml['cluster'] = kmeans.fit_predict(X_cluster_scaled)\n",
    "    \n",
    "    # An√°lise dos clusters\n",
    "    print(\"\\nüìä CARACTER√çSTICAS DOS CLUSTERS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    cluster_analysis = df_ml.groupby('cluster').agg({\n",
    "        'cnpj': 'count',\n",
    "        'valor_total_infracao': ['mean', 'sum', 'std'],\n",
    "        'valor_total_nf': ['mean', 'sum'],\n",
    "        'gerou_notificacao': ['mean', 'sum'],\n",
    "        'ciclo_completo': 'mean',\n",
    "        'dias_infracao_ate_nf': 'mean',\n",
    "        'pagou_dp_infracao': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(cluster_analysis)\n",
    "    \n",
    "    # Distribui√ß√£o dos clusters\n",
    "    cluster_counts = df_ml['cluster'].value_counts().sort_index()\n",
    "    \n",
    "    fig_dist = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=cluster_counts.index,\n",
    "            y=cluster_counts.values,\n",
    "            text=cluster_counts.values,\n",
    "            textposition='auto',\n",
    "            marker_color='lightseagreen'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_dist.update_layout(\n",
    "        title='Distribui√ß√£o de Fiscaliza√ß√µes por Cluster',\n",
    "        xaxis_title='Cluster',\n",
    "        yaxis_title='Quantidade de Fiscaliza√ß√µes',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig_dist.show()\n",
    "    \n",
    "    # PCA para visualiza√ß√£o 2D\n",
    "    print(\"\\nüîÑ Aplicando PCA para visualiza√ß√£o 2D...\")\n",
    "    \n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_cluster_scaled)\n",
    "    \n",
    "    df_ml['pca1'] = X_pca[:, 0]\n",
    "    df_ml['pca2'] = X_pca[:, 1]\n",
    "    \n",
    "    print(f\"üìä Vari√¢ncia explicada: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "    \n",
    "    # Visualiza√ß√£o dos clusters no espa√ßo PCA\n",
    "    n_sample = int(np.minimum(5000, len(df_ml)))\n",
    "    df_sample = df_ml.sample(n=n_sample, random_state=42)\n",
    "    \n",
    "    fig_pca = px.scatter(\n",
    "        df_sample,\n",
    "        x='pca1',\n",
    "        y='pca2',\n",
    "        color='cluster',\n",
    "        hover_data=['valor_total_infracao', 'regime_tributario', 'cnae_secao_descricao'],\n",
    "        title='Visualiza√ß√£o dos Clusters no Espa√ßo PCA',\n",
    "        labels={'pca1': f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "                'pca2': f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)'},\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig_pca.update_traces(marker=dict(size=5, opacity=0.6))\n",
    "    fig_pca.update_layout(height=600)\n",
    "    fig_pca.show()\n",
    "    \n",
    "    # Perfil de cada cluster\n",
    "    print(\"\\nüéØ PERFIL DOS CLUSTERS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for cluster_id in sorted(df_ml['cluster'].unique()):\n",
    "        cluster_data = df_ml[df_ml['cluster'] == cluster_id]\n",
    "        \n",
    "        print(f\"\\nüîπ CLUSTER {cluster_id} ({len(cluster_data):,} fiscaliza√ß√µes)\")\n",
    "        print(f\"  Valor m√©dio infra√ß√£o: {format_currency(cluster_data['valor_total_infracao'].mean())}\")\n",
    "        print(f\"  Taxa notifica√ß√£o: {cluster_data['gerou_notificacao'].mean()*100:.2f}%\")\n",
    "        print(f\"  Taxa ciclo completo: {cluster_data['ciclo_completo'].mean()*100:.2f}%\")\n",
    "        print(f\"  M√©dia dias at√© NF: {cluster_data['dias_infracao_ate_nf'].mean():.0f} dias\")\n",
    "        \n",
    "        # Top 3 setores\n",
    "        top_setores = cluster_data['cnae_secao_descricao'].value_counts().head(3)\n",
    "        print(f\"  Top 3 setores:\")\n",
    "        for setor, qtd in top_setores.items():\n",
    "            print(f\"    - {setor}: {qtd} ({qtd/len(cluster_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Salvar resultados\n",
    "    cluster_results = {\n",
    "        'model': kmeans,\n",
    "        'scaler': scaler_cluster,\n",
    "        'pca': pca,\n",
    "        'k_ideal': k_ideal,\n",
    "        'silhouette': max_silhouette,\n",
    "        'df_clustered': df_ml\n",
    "    }\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de clusters conclu√≠da!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados ML n√£o dispon√≠veis\")\n",
    "    cluster_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6388d-7a09-4c93-896e-cf2949fbe387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 13: AN√ÅLISE DE DEFESA PR√âVIA E PAGAMENTOS\n",
    "# ============================================================================\n",
    "\n",
    "if not df_fisc.empty:\n",
    "    \n",
    "    print(\"üí∞ AN√ÅLISE DE DEFESA PR√âVIA E PAGAMENTOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # An√°lise de pagamentos em DP\n",
    "    total_fiscalizacoes = len(df_fisc)\n",
    "    pagou_dp = df_fisc['pagou_dp_infracao'].sum()\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DE DEFESA PR√âVIA:\")\n",
    "    print(f\"  Total de fiscaliza√ß√µes: {total_fiscalizacoes:,}\")\n",
    "    print(f\"  Pagamentos em DP: {pagou_dp:,}\")\n",
    "    print(f\"  Taxa de pagamento DP: {pagou_dp/total_fiscalizacoes*100:.2f}%\")\n",
    "    \n",
    "    # An√°lise por ano\n",
    "    dp_por_ano = df_fisc.groupby('ano_infracao').agg({\n",
    "        'cnpj': 'count',\n",
    "        'pagou_dp_infracao': 'sum',\n",
    "        'valor_total_infracao': 'sum',\n",
    "        'gerou_notificacao': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dp_por_ano.columns = ['ano', 'total_fisc', 'pagamentos_dp', 'valor_total', 'notificacoes']\n",
    "    dp_por_ano['taxa_dp'] = dp_por_ano['pagamentos_dp'] / dp_por_ano['total_fisc'] * 100\n",
    "    dp_por_ano['taxa_notif'] = dp_por_ano['notificacoes'] / dp_por_ano['total_fisc'] * 100\n",
    "    \n",
    "    print(\"\\nüìä Evolu√ß√£o de Pagamentos DP por Ano:\")\n",
    "    print(dp_por_ano)\n",
    "    \n",
    "    # Visualiza√ß√£o - Evolu√ß√£o DP\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Quantidade de Pagamentos em Defesa Pr√©via', 'Taxa de Pagamento DP vs Taxa de Notifica√ß√£o'),\n",
    "        specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Gr√°fico 1: Quantidade\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=dp_por_ano['ano'],\n",
    "            y=dp_por_ano['pagamentos_dp'],\n",
    "            name='Pagamentos DP',\n",
    "            marker_color='lightcoral',\n",
    "            text=dp_por_ano['pagamentos_dp'],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Gr√°fico 2: Taxas\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=dp_por_ano['ano'],\n",
    "            y=dp_por_ano['taxa_dp'],\n",
    "            name='Taxa Pagamento DP',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='red', width=3)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=dp_por_ano['ano'],\n",
    "            y=dp_por_ano['taxa_notif'],\n",
    "            name='Taxa Notifica√ß√£o',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='blue', width=3)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Ano\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Quantidade\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Taxa (%)\", row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        title_text=\"An√°lise de Defesa Pr√©via ao Longo do Tempo\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # An√°lise: Pagamento DP vs Valor\n",
    "    print(\"\\nüíµ AN√ÅLISE POR FAIXA DE VALOR:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_fisc['faixa_valor'] = pd.cut(\n",
    "        df_fisc['valor_total_infracao'],\n",
    "        bins=[0, 50000, 100000, 500000, 1000000, np.inf],\n",
    "        labels=['< 50k', '50k-100k', '100k-500k', '500k-1M', '> 1M']\n",
    "    )\n",
    "    \n",
    "    dp_por_valor = df_fisc.groupby('faixa_valor').agg({\n",
    "        'cnpj': 'count',\n",
    "        'pagou_dp_infracao': 'sum',\n",
    "        'valor_total_infracao': ['mean', 'sum'],\n",
    "        'gerou_notificacao': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dp_por_valor.columns = ['faixa_valor', 'total', 'pagamentos_dp', 'valor_medio', 'valor_total', 'notificacoes']\n",
    "    dp_por_valor['taxa_dp'] = dp_por_valor['pagamentos_dp'] / dp_por_valor['total'] * 100\n",
    "    dp_por_valor['taxa_notif'] = dp_por_valor['notificacoes'] / dp_por_valor['total'] * 100\n",
    "    \n",
    "    print(dp_por_valor)\n",
    "    \n",
    "    # Visualiza√ß√£o por faixa de valor\n",
    "    fig2 = go.Figure()\n",
    "    \n",
    "    fig2.add_trace(go.Bar(\n",
    "        x=dp_por_valor['faixa_valor'],\n",
    "        y=dp_por_valor['taxa_dp'],\n",
    "        name='Taxa Pagamento DP',\n",
    "        marker_color='salmon',\n",
    "        text=dp_por_valor['taxa_dp'].round(2),\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig2.add_trace(go.Bar(\n",
    "        x=dp_por_valor['faixa_valor'],\n",
    "        y=dp_por_valor['taxa_notif'],\n",
    "        name='Taxa Notifica√ß√£o',\n",
    "        marker_color='steelblue',\n",
    "        text=dp_por_valor['taxa_notif'].round(2),\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title='Taxa de Pagamento DP vs Notifica√ß√£o por Faixa de Valor',\n",
    "        xaxis_title='Faixa de Valor da Infra√ß√£o',\n",
    "        yaxis_title='Taxa (%)',\n",
    "        barmode='group',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig2.show()\n",
    "    \n",
    "    # An√°lise por regime tribut√°rio\n",
    "    print(\"\\nüèõÔ∏è AN√ÅLISE POR REGIME TRIBUT√ÅRIO:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    dp_por_regime = df_fisc.groupby('regime_tributario').agg({\n",
    "        'cnpj': 'count',\n",
    "        'pagou_dp_infracao': 'sum',\n",
    "        'gerou_notificacao': 'sum',\n",
    "        'valor_total_infracao': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dp_por_regime.columns = ['regime', 'total', 'pagamentos_dp', 'notificacoes', 'valor_total']\n",
    "    dp_por_regime['taxa_dp'] = dp_por_regime['pagamentos_dp'] / dp_por_regime['total'] * 100\n",
    "    dp_por_regime['taxa_notif'] = dp_por_regime['notificacoes'] / dp_por_regime['total'] * 100\n",
    "    dp_por_regime = dp_por_regime.sort_values('total', ascending=False)\n",
    "    \n",
    "    print(dp_por_regime.head(10))\n",
    "    \n",
    "    # Visualiza√ß√£o por regime\n",
    "    top_regimes = dp_por_regime.head(8)\n",
    "    \n",
    "    fig3 = px.bar(\n",
    "        top_regimes,\n",
    "        x='regime',\n",
    "        y=['taxa_dp', 'taxa_notif'],\n",
    "        title='Compara√ß√£o Taxa DP vs Notifica√ß√£o por Regime Tribut√°rio (Top 8)',\n",
    "        labels={'value': 'Taxa (%)', 'variable': 'M√©trica'},\n",
    "        barmode='group',\n",
    "        color_discrete_map={'taxa_dp': 'coral', 'taxa_notif': 'teal'}\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(height=500, xaxis_tickangle=-45)\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de Defesa Pr√©via conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame vazio - pulando an√°lise de DP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802b3fe-86bc-4511-94f9-bbfb5f3c5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 14: AN√ÅLISE DE EFETIVIDADE - SCORES (COMPLETA)\n",
    "# ============================================================================\n",
    "\n",
    "query_scores = \"\"\"\n",
    "SELECT \n",
    "    cnpj,\n",
    "    nm_razao_social,\n",
    "    gerfe,\n",
    "    municipio,\n",
    "    cnae_secao_descricao,\n",
    "    regime_tributario,\n",
    "    ano_infracao,\n",
    "    CAST(COALESCE(valor_total_infracao, 0) AS DOUBLE) AS valor_total_infracao,\n",
    "    CAST(COALESCE(valor_total_nf, 0) AS DOUBLE) AS valor_total_nf,\n",
    "    CAST(COALESCE(gerou_notificacao, 0) AS INT) AS gerou_notificacao,\n",
    "    CAST(COALESCE(ciclo_completo, 0) AS INT) AS ciclo_completo,\n",
    "    CAST(COALESCE(dias_infracao_ate_nf, 0) AS DOUBLE) AS dias_infracao_ate_nf,\n",
    "    CAST(COALESCE(score_efetividade_final, 0) AS DOUBLE) AS score_efetividade_final,\n",
    "    classificacao_efetividade,\n",
    "    CAST(COALESCE(score_geracao_nf, 0) AS DOUBLE) AS score_geracao_nf,\n",
    "    CAST(COALESCE(score_ciclo, 0) AS DOUBLE) AS score_ciclo,\n",
    "    CAST(COALESCE(score_valor_notificado, 0) AS DOUBLE) AS score_valor_notificado,\n",
    "    CAST(COALESCE(score_tempestividade, 0) AS DOUBLE) AS score_tempestividade\n",
    "FROM teste.fisca_scores_efetividade\n",
    "WHERE ano_infracao >= 2020\n",
    "\"\"\"\n",
    "\n",
    "df_scores = load_spark_to_pandas_safe(\n",
    "    query_scores,\n",
    "    limit=150000,\n",
    "    view_name=\"vw_scores\"\n",
    ")\n",
    "\n",
    "if not df_scores.empty:\n",
    "    \n",
    "    print(\"üìä AN√ÅLISE DE SCORES DE EFETIVIDADE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Estat√≠sticas dos scores\n",
    "    print(\"\\nüìà ESTAT√çSTICAS DOS SCORES:\")\n",
    "    print(f\"  Score M√©dio Geral: {df_scores['score_efetividade_final'].mean():.2f}\")\n",
    "    print(f\"  Score Mediano: {df_scores['score_efetividade_final'].median():.2f}\")\n",
    "    print(f\"  Desvio Padr√£o: {df_scores['score_efetividade_final'].std():.2f}\")\n",
    "    print(f\"  Score M√≠nimo: {df_scores['score_efetividade_final'].min():.2f}\")\n",
    "    print(f\"  Score M√°ximo: {df_scores['score_efetividade_final'].max():.2f}\")\n",
    "    \n",
    "    # Distribui√ß√£o por classifica√ß√£o\n",
    "    print(\"\\nüéØ DISTRIBUI√á√ÉO POR CLASSIFICA√á√ÉO:\")\n",
    "    class_dist = df_scores['classificacao_efetividade'].value_counts()\n",
    "    print(class_dist)\n",
    "    \n",
    "    # Visualiza√ß√£o - Distribui√ß√£o de Scores\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Distribui√ß√£o do Score Final',\n",
    "            'Distribui√ß√£o por Classifica√ß√£o',\n",
    "            'Scores por Componente',\n",
    "            'Evolu√ß√£o do Score ao Longo do Tempo'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "            [{\"type\": \"box\"}, {\"type\": \"scatter\"}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 1. Histograma do Score Final\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df_scores['score_efetividade_final'],\n",
    "            nbinsx=50,\n",
    "            name='Score Final',\n",
    "            marker_color='steelblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Distribui√ß√£o por Classifica√ß√£o\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=class_dist.index,\n",
    "            y=class_dist.values,\n",
    "            name='Classifica√ß√£o',\n",
    "            marker_color='coral',\n",
    "            text=class_dist.values,\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Box plot dos componentes\n",
    "    score_components = ['score_geracao_nf', 'score_ciclo', 'score_valor_notificado', 'score_tempestividade']\n",
    "    for comp in score_components:\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=df_scores[comp],\n",
    "                name=comp.replace('score_', '').replace('_', ' ').title(),\n",
    "                boxmean='sd'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Evolu√ß√£o temporal\n",
    "    score_por_ano = df_scores.groupby('ano_infracao')['score_efetividade_final'].mean().reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=score_por_ano['ano_infracao'],\n",
    "            y=score_por_ano['score_efetividade_final'],\n",
    "            mode='lines+markers',\n",
    "            name='Score M√©dio',\n",
    "            line=dict(color='green', width=3),\n",
    "            marker=dict(size=10)\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        showlegend=True,\n",
    "        title_text=\"üìä An√°lise Completa de Scores de Efetividade\"\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Score\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Classifica√ß√£o\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Ano\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Frequ√™ncia\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Quantidade\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score M√©dio\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # An√°lise por Ger√™ncia\n",
    "    print(\"\\nüè¢ TOP 10 GER√äNCIAS POR SCORE:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    score_por_gerencia = df_scores.groupby('gerfe').agg({\n",
    "        'cnpj': 'count',\n",
    "        'score_efetividade_final': 'mean',\n",
    "        'valor_total_nf': 'sum',\n",
    "        'gerou_notificacao': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    score_por_gerencia.columns = ['gerencia', 'qtd_fisc', 'score_medio', 'valor_total_nf', 'total_nfs']\n",
    "    score_por_gerencia = score_por_gerencia.sort_values('score_medio', ascending=False)\n",
    "    \n",
    "    print(score_por_gerencia.head(10))\n",
    "    \n",
    "    # Visualiza√ß√£o Ger√™ncias\n",
    "    top_gerencias = score_por_gerencia.head(15)\n",
    "    \n",
    "    fig2 = px.bar(\n",
    "        top_gerencias,\n",
    "        x='gerencia',\n",
    "        y='score_medio',\n",
    "        title='Top 15 Ger√™ncias por Score de Efetividade',\n",
    "        labels={'score_medio': 'Score M√©dio', 'gerencia': 'Ger√™ncia'},\n",
    "        color='score_medio',\n",
    "        color_continuous_scale='RdYlGn',\n",
    "        text='score_medio'\n",
    "    )\n",
    "    \n",
    "    fig2.update_traces(texttemplate='%{text:.1f}', textposition='outside')\n",
    "    fig2.update_layout(height=600, xaxis_tickangle=-45)\n",
    "    fig2.show()\n",
    "    \n",
    "    # An√°lise por Setor\n",
    "    print(\"\\nüè≠ TOP 10 SETORES POR SCORE:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    score_por_setor = df_scores.groupby('cnae_secao_descricao').agg({\n",
    "        'cnpj': 'count',\n",
    "        'score_efetividade_final': 'mean',\n",
    "        'valor_total_nf': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    score_por_setor.columns = ['setor', 'qtd_fisc', 'score_medio', 'valor_total_nf']\n",
    "    score_por_setor = score_por_setor[score_por_setor['qtd_fisc'] >= 20]  # M√≠nimo 20 fiscaliza√ß√µes\n",
    "    score_por_setor = score_por_setor.sort_values('score_medio', ascending=False)\n",
    "    \n",
    "    print(score_por_setor.head(10))\n",
    "    \n",
    "    # Scatter plot: Score vs Valor - CORRIGIDO\n",
    "    n_sample_scores = int(np.minimum(5000, len(df_scores)))\n",
    "    df_scores_sample = df_scores.sample(n=n_sample_scores, random_state=42)\n",
    "    \n",
    "    fig3 = px.scatter(\n",
    "        df_scores_sample,\n",
    "        x='valor_total_infracao',\n",
    "        y='score_efetividade_final',\n",
    "        color='classificacao_efetividade',\n",
    "        size='valor_total_nf',\n",
    "        hover_data=['nm_razao_social', 'municipio'],\n",
    "        title='Rela√ß√£o entre Valor da Infra√ß√£o e Score de Efetividade',\n",
    "        labels={\n",
    "            'valor_total_infracao': 'Valor Total Infra√ß√£o (R$)',\n",
    "            'score_efetividade_final': 'Score de Efetividade'\n",
    "        },\n",
    "        color_discrete_map={\n",
    "            'MUITO EFETIVA': 'green',\n",
    "            'EFETIVA': 'lightgreen',\n",
    "            'MODERADAMENTE EFETIVA': 'yellow',\n",
    "            'POUCO EFETIVA': 'red'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(height=600)\n",
    "    fig3.update_xaxes(type=\"log\")\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de efetividade conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame vazio - pulando an√°lise de scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb2bf8-0d05-4b96-9aed-30f3d938659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 15: AN√ÅLISE DE PRODUTIVIDADE POR AFRE\n",
    "# ============================================================================\n",
    "\n",
    "query_afres = \"\"\"\n",
    "SELECT \n",
    "    matricula_afre,\n",
    "    nome_afre,\n",
    "    ano,\n",
    "    CAST(COALESCE(meses_ativos, 0) AS DOUBLE) AS meses_ativos,\n",
    "    CAST(COALESCE(dias_ativos, 0) AS DOUBLE) AS dias_ativos,\n",
    "    CAST(COALESCE(qtd_infracoes, 0) AS DOUBLE) AS qtd_infracoes,\n",
    "    CAST(COALESCE(qtd_empresas_fiscalizadas, 0) AS DOUBLE) AS qtd_empresas_fiscalizadas,\n",
    "    CAST(COALESCE(qtd_infracoes_com_ciencia, 0) AS DOUBLE) AS qtd_infracoes_com_ciencia,\n",
    "    CAST(COALESCE(valor_total_infracoes, 0) AS DOUBLE) AS valor_total_infracoes,\n",
    "    CAST(COALESCE(qtd_nfs, 0) AS DOUBLE) AS qtd_nfs,\n",
    "    CAST(COALESCE(valor_total_lancado, 0) AS DOUBLE) AS valor_total_lancado,\n",
    "    CAST(COALESCE(infracoes_por_mes, 0) AS DOUBLE) AS infracoes_por_mes,\n",
    "    CAST(COALESCE(nfs_por_mes, 0) AS DOUBLE) AS nfs_por_mes,\n",
    "    CAST(COALESCE(taxa_conversao_infracao_nf, 0) AS DOUBLE) AS taxa_conversao_infracao_nf,\n",
    "    CAST(COALESCE(valor_medio_mensal, 0) AS DOUBLE) AS valor_medio_mensal\n",
    "FROM teste.fisca_metricas_por_afre\n",
    "WHERE meses_ativos >= 6\n",
    "  AND ano >= 2020\n",
    "\"\"\"\n",
    "\n",
    "df_afres = load_spark_to_pandas_safe(\n",
    "    query_afres,\n",
    "    limit=100000,\n",
    "    view_name=\"vw_afres\"\n",
    ")\n",
    "\n",
    "if not df_afres.empty:\n",
    "    \n",
    "    print(\"üë• AN√ÅLISE DE PRODUTIVIDADE POR AFRE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Ano mais recente\n",
    "    ano_recente = df_afres['ano'].max()\n",
    "    df_afres_recente = df_afres[df_afres['ano'] == ano_recente]\n",
    "    \n",
    "    print(f\"\\nüìÖ An√°lise do ano: {ano_recente}\")\n",
    "    print(f\"  Total de AFREs ativos: {df_afres_recente['matricula_afre'].nunique():,}\")\n",
    "    print(f\"  Total de infra√ß√µes: {df_afres_recente['qtd_infracoes'].sum():,.0f}\")\n",
    "    print(f\"  Total de NFs: {df_afres_recente['qtd_nfs'].sum():,.0f}\")\n",
    "    print(f\"  Valor total lan√ßado: {format_currency(df_afres_recente['valor_total_lancado'].sum())}\")\n",
    "    \n",
    "    # Estat√≠sticas de produtividade\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DE PRODUTIVIDADE ({ano_recente}):\")\n",
    "    print(f\"  Infra√ß√µes/m√™s - M√©dia: {df_afres_recente['infracoes_por_mes'].mean():.2f}\")\n",
    "    print(f\"  Infra√ß√µes/m√™s - Mediana: {df_afres_recente['infracoes_por_mes'].median():.2f}\")\n",
    "    print(f\"  NFs/m√™s - M√©dia: {df_afres_recente['nfs_por_mes'].mean():.2f}\")\n",
    "    print(f\"  NFs/m√™s - Mediana: {df_afres_recente['nfs_por_mes'].median():.2f}\")\n",
    "    print(f\"  Taxa convers√£o - M√©dia: {df_afres_recente['taxa_conversao_infracao_nf'].mean():.2f}%\")\n",
    "    \n",
    "    # Top 20 AFREs mais produtivos\n",
    "    top_afres = df_afres_recente.nlargest(20, 'valor_total_lancado')\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP 20 AFREs POR VALOR LAN√áADO ({ano_recente}):\")\n",
    "    print(top_afres[['nome_afre', 'qtd_nfs', 'valor_total_lancado', 'nfs_por_mes', 'taxa_conversao_infracao_nf']])\n",
    "    \n",
    "    # Visualiza√ß√£o - Top 20 AFREs\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=(\n",
    "            f'Top 20 AFREs por Valor Lan√ßado ({ano_recente})',\n",
    "            f'Top 20 AFREs por Produtividade (NFs/m√™s) ({ano_recente})'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Gr√°fico 1: Valor lan√ßado\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_afres['nome_afre'],\n",
    "            y=top_afres['valor_total_lancado'] / 1_000_000,\n",
    "            name='Valor Lan√ßado',\n",
    "            marker_color='mediumseagreen',\n",
    "            text=(top_afres['valor_total_lancado'] / 1_000_000).round(2),\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Gr√°fico 2: NFs por m√™s\n",
    "    top_afres_prod = df_afres_recente.nlargest(20, 'nfs_por_mes')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_afres_prod['nome_afre'],\n",
    "            y=top_afres_prod['nfs_por_mes'],\n",
    "            name='NFs/m√™s',\n",
    "            marker_color='lightsalmon',\n",
    "            text=top_afres_prod['nfs_por_mes'].round(2),\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(tickangle=-45, row=1, col=1)\n",
    "    fig.update_xaxes(tickangle=-45, row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"R$ Milh√µes\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"NFs/m√™s\", row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(height=900, showlegend=False, title_text=\"üìä Produtividade dos AFREs\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Distribui√ß√£o de produtividade\n",
    "    print(f\"\\nüìä DISTRIBUI√á√ÉO DE PRODUTIVIDADE ({ano_recente}):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_afres_recente['faixa_produtividade'] = pd.cut(\n",
    "        df_afres_recente['nfs_por_mes'],\n",
    "        bins=[0, 0.5, 1, 2, 3, np.inf],\n",
    "        labels=['Muito Baixa (<0.5)', 'Baixa (0.5-1)', 'M√©dia (1-2)', 'Alta (2-3)', 'Muito Alta (>3)']\n",
    "    )\n",
    "    \n",
    "    dist_prod = df_afres_recente['faixa_produtividade'].value_counts().sort_index()\n",
    "    print(dist_prod)\n",
    "    \n",
    "    # Visualiza√ß√£o distribui√ß√£o\n",
    "    fig2 = go.Figure(data=[\n",
    "        go.Pie(\n",
    "            labels=dist_prod.index,\n",
    "            values=dist_prod.values,\n",
    "            hole=0.4,\n",
    "            marker=dict(colors=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title=f'Distribui√ß√£o de AFREs por Faixa de Produtividade ({ano_recente})',\n",
    "        height=500\n",
    "    )\n",
    "    fig2.show()\n",
    "    \n",
    "    # Evolu√ß√£o temporal da produtividade\n",
    "    print(\"\\nüìà EVOLU√á√ÉO TEMPORAL DA PRODUTIVIDADE:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    prod_temporal = df_afres.groupby('ano').agg({\n",
    "        'matricula_afre': 'count',\n",
    "        'qtd_infracoes': 'sum',\n",
    "        'qtd_nfs': 'sum',\n",
    "        'valor_total_lancado': 'sum',\n",
    "        'infracoes_por_mes': 'mean',\n",
    "        'nfs_por_mes': 'mean',\n",
    "        'taxa_conversao_infracao_nf': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    prod_temporal.columns = [\n",
    "        'ano', 'qtd_afres', 'total_infracoes', 'total_nfs',\n",
    "        'valor_total', 'media_infracoes_mes', 'media_nfs_mes', 'taxa_conversao'\n",
    "    ]\n",
    "    \n",
    "    print(prod_temporal)\n",
    "    \n",
    "    # Visualiza√ß√£o evolu√ß√£o\n",
    "    fig3 = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Evolu√ß√£o Quantidade de AFREs',\n",
    "            'Evolu√ß√£o Total de NFs',\n",
    "            'Evolu√ß√£o Produtividade M√©dia (NFs/m√™s)',\n",
    "            'Evolu√ß√£o Taxa de Convers√£o M√©dia'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(x=prod_temporal['ano'], y=prod_temporal['qtd_afres'],\n",
    "                   mode='lines+markers', name='AFREs', line=dict(color='blue', width=3)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(x=prod_temporal['ano'], y=prod_temporal['total_nfs'],\n",
    "                   mode='lines+markers', name='Total NFs', line=dict(color='green', width=3)),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(x=prod_temporal['ano'], y=prod_temporal['media_nfs_mes'],\n",
    "                   mode='lines+markers', name='NFs/m√™s', line=dict(color='orange', width=3)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(x=prod_temporal['ano'], y=prod_temporal['taxa_conversao'],\n",
    "                   mode='lines+markers', name='Taxa Convers√£o', line=dict(color='red', width=3)),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(height=800, showlegend=False, title_text=\"üìä Evolu√ß√£o Temporal da Produtividade\")\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de produtividade por AFRE conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è DataFrame vazio - pulando an√°lise de AFREs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0b054-21b0-4383-8b77-1061db1db65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 16: AN√ÅLISE DO FUNIL DE FISCALIZA√á√ÉO\n",
    "# ============================================================================\n",
    "# Baseado na Portaria SEF 031/2021 e Ato DIAT 004/2021\n",
    "# Monitoramento ‚Üí Acompanhamento ‚Üí PAF (Constitui√ß√£o do Cr√©dito)\n",
    "\n",
    "if not df_fisc.empty:\n",
    "    \n",
    "    print(\"üîç AN√ÅLISE DO FUNIL DE FISCALIZA√á√ÉO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Baseado na Portaria SEF 031/2021:\")\n",
    "    print(\"  I - Monitoramento (observa√ß√£o permanente)\")\n",
    "    print(\"  II - Acompanhamento (inconsist√™ncias detectadas)\")\n",
    "    print(\"  III - PAF (Constitui√ß√£o do Cr√©dito Tribut√°rio)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Carregar dados de acompanhamentos\n",
    "    query_acomp = \"\"\"\n",
    "    SELECT \n",
    "        ano_os,\n",
    "        COUNT(DISTINCT id_acompanhamento) AS qtd_acompanhamentos,\n",
    "        COUNT(DISTINCT cnpj) AS empresas_acompanhadas,\n",
    "        SUM(CASE WHEN gerou_documento = 1 THEN 1 ELSE 0 END) AS acomp_com_documento,\n",
    "        SUM(CASE WHEN foi_encerrado = 1 THEN 1 ELSE 0 END) AS acomp_encerrados,\n",
    "        AVG(CASE WHEN dias_duracao_acao > 0 THEN dias_duracao_acao END) AS media_dias_duracao\n",
    "    FROM teste.fisca_acompanhamentos\n",
    "    WHERE ano_os >= 2020\n",
    "    GROUP BY ano_os\n",
    "    ORDER BY ano_os DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    df_acomp_resumo = load_spark_to_pandas_safe(\n",
    "        query_acomp,\n",
    "        view_name=\"vw_acomp_resumo\"\n",
    "    )\n",
    "    \n",
    "    if not df_acomp_resumo.empty:\n",
    "        print(\"\\nüìä RESUMO DE ACOMPANHAMENTOS POR ANO:\")\n",
    "        print(df_acomp_resumo)\n",
    "        \n",
    "        # Calcular convers√£o Acompanhamento ‚Üí PAF\n",
    "        funil_por_ano = pd.merge(\n",
    "            df_acomp_resumo[['ano_os', 'qtd_acompanhamentos', 'empresas_acompanhadas']],\n",
    "            df_dashboard[['ano', 'qtd_infracoes_lavradas', 'empresas_fiscalizadas']],\n",
    "            left_on='ano_os',\n",
    "            right_on='ano',\n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        funil_por_ano['taxa_conversao_acomp_paf'] = np.where(\n",
    "            funil_por_ano['qtd_acompanhamentos'] > 0,\n",
    "            funil_por_ano['qtd_infracoes_lavradas'] / funil_por_ano['qtd_acompanhamentos'] * 100,\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüìä FUNIL: ACOMPANHAMENTO ‚Üí PAF (CONSTITUI√á√ÉO DO CR√âDITO):\")\n",
    "        print(\"=\" * 80)\n",
    "        print(funil_por_ano[['ano', 'qtd_acompanhamentos', 'qtd_infracoes_lavradas', 'taxa_conversao_acomp_paf']])\n",
    "        \n",
    "        # Visualiza√ß√£o do Funil\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        anos = funil_por_ano['ano'].astype(int)\n",
    "        \n",
    "        # Funil invertido\n",
    "        fig.add_trace(go.Funnel(\n",
    "            name='2020',\n",
    "            y=['Acompanhamentos', 'PAF (Infra√ß√µes)', 'Notifica√ß√µes Fiscais', 'Ciclos Completos'],\n",
    "            x=[\n",
    "                df_acomp_resumo[df_acomp_resumo['ano_os'] == 2020]['qtd_acompanhamentos'].sum(),\n",
    "                df_dashboard[df_dashboard['ano'] == 2020]['qtd_infracoes_lavradas'].sum(),\n",
    "                df_dashboard[df_dashboard['ano'] == 2020]['qtd_nfs_emitidas'].sum(),\n",
    "                df_dashboard[df_dashboard['ano'] == 2020]['qtd_ciclos_completos'].sum()\n",
    "            ],\n",
    "            textinfo=\"value+percent initial\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Funil de Fiscaliza√ß√£o 2020: Acompanhamento ‚Üí PAF ‚Üí NF ‚Üí Ciclo Completo',\n",
    "            height=500\n",
    "        )\n",
    "        fig.show()\n",
    "        \n",
    "        # Evolu√ß√£o temporal do funil\n",
    "        fig2 = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=(\n",
    "                'Evolu√ß√£o: Acompanhamentos vs PAF (Infra√ß√µes)',\n",
    "                'Taxa de Convers√£o Acompanhamento ‚Üí PAF'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig2.add_trace(\n",
    "            go.Bar(\n",
    "                x=funil_por_ano['ano'],\n",
    "                y=funil_por_ano['qtd_acompanhamentos'],\n",
    "                name='Acompanhamentos',\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig2.add_trace(\n",
    "            go.Bar(\n",
    "                x=funil_por_ano['ano'],\n",
    "                y=funil_por_ano['qtd_infracoes_lavradas'],\n",
    "                name='PAF (Infra√ß√µes)',\n",
    "                marker_color='indianred'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig2.add_trace(\n",
    "            go.Scatter(\n",
    "                x=funil_por_ano['ano'],\n",
    "                y=funil_por_ano['taxa_conversao_acomp_paf'],\n",
    "                name='Taxa Convers√£o',\n",
    "                mode='lines+markers',\n",
    "                line=dict(color='green', width=3),\n",
    "                marker=dict(size=10)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig2.update_xaxes(title_text=\"Ano\", row=2, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Quantidade\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Taxa (%)\", row=2, col=1)\n",
    "        \n",
    "        fig2.update_layout(height=800, title_text=\"An√°lise do Funil de Fiscaliza√ß√£o\")\n",
    "        fig2.show()\n",
    "    \n",
    "    # An√°lise de efetividade do acompanhamento\n",
    "    print(\"\\nüéØ EFETIVIDADE DO ACOMPANHAMENTO:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Conforme Portaria SEF 031/2021, Art. 6¬∫, ¬ß 2¬∫:\")\n",
    "    print(\"'As a√ß√µes fiscais auxiliares de acompanhamento podem ser utilizadas para\")\n",
    "    print(\"propor ao sujeito passivo que, espontaneamente, regularize ou preste\")\n",
    "    print(\"esclarecimento sobre inconsist√™ncias detectadas.'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calcular empresas que regularizaram espontaneamente\n",
    "    # (empresas com acompanhamento mas SEM PAF constitu√≠do)\n",
    "    query_regularizacao = \"\"\"\n",
    "    SELECT \n",
    "        a.ano_os,\n",
    "        COUNT(DISTINCT a.cnpj) AS empresas_acompanhadas,\n",
    "        COUNT(DISTINCT CASE WHEN f.cnpj IS NULL THEN a.cnpj END) AS empresas_sem_paf,\n",
    "        COUNT(DISTINCT CASE WHEN f.cnpj IS NOT NULL THEN a.cnpj END) AS empresas_com_paf\n",
    "    FROM teste.fisca_acompanhamentos a\n",
    "    LEFT JOIN teste.fisca_fiscalizacoes_consolidadas f\n",
    "        ON a.cnpj = f.cnpj\n",
    "        AND a.ano_os = f.ano_infracao\n",
    "    WHERE a.ano_os >= 2020\n",
    "        AND a.cnpj IS NOT NULL\n",
    "    GROUP BY a.ano_os\n",
    "    ORDER BY a.ano_os DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    df_reg = load_spark_to_pandas_safe(query_regularizacao, view_name=\"vw_regularizacao\")\n",
    "    \n",
    "    if not df_reg.empty:\n",
    "        df_reg['taxa_regularizacao_espontanea'] = (\n",
    "            df_reg['empresas_sem_paf'] / df_reg['empresas_acompanhadas'] * 100\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüìä ESTIMATIVA DE REGULARIZA√á√ÉO ESPONT√ÇNEA:\")\n",
    "        print(\"(Empresas com acompanhamento que N√ÉO geraram PAF)\")\n",
    "        print(df_reg)\n",
    "        \n",
    "        # Visualiza√ß√£o\n",
    "        fig3 = px.bar(\n",
    "            df_reg,\n",
    "            x='ano_os',\n",
    "            y=['empresas_sem_paf', 'empresas_com_paf'],\n",
    "            title='Regulariza√ß√£o Espont√¢nea vs Constitui√ß√£o de PAF',\n",
    "            labels={'value': 'Quantidade de Empresas', 'ano_os': 'Ano'},\n",
    "            barmode='stack',\n",
    "            color_discrete_map={\n",
    "                'empresas_sem_paf': 'lightgreen',\n",
    "                'empresas_com_paf': 'salmon'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        fig3.update_layout(\n",
    "            height=500,\n",
    "            legend_title_text='Situa√ß√£o',\n",
    "            yaxis_title=\"Empresas\"\n",
    "        )\n",
    "        fig3.show()\n",
    "        \n",
    "        print(\"\\nüí° INSIGHT:\")\n",
    "        print(f\"Taxa m√©dia de regulariza√ß√£o espont√¢nea: {df_reg['taxa_regularizacao_espontanea'].mean():.2f}%\")\n",
    "        print(\"Empresas que receberam acompanhamento e regularizaram SEM necessidade de PAF\")\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise do funil de fiscaliza√ß√£o conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados insuficientes para an√°lise do funil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fa6b4-33cf-44af-af44-512abd0d72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 17: AN√ÅLISE DE TEMPESTIVIDADE E SLA (Service Level Agreement)\n",
    "# ============================================================================\n",
    "\n",
    "if not df_fisc.empty:\n",
    "    \n",
    "    print(\"‚è±Ô∏è AN√ÅLISE DE TEMPESTIVIDADE DAS A√á√ïES FISCAIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Baseado no Ato DIAT 004/2021, Art. 3¬∫, ¬ß 1¬∫, III:\")\n",
    "    print(\"'O termo de instaura√ß√£o do PAF conter√° o prazo de execu√ß√£o da a√ß√£o fiscal'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # An√°lise de tempos\n",
    "    tempos_analysis = df_fisc[df_fisc['dias_infracao_ate_nf'] > 0].copy()\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DE TEMPO (Infra√ß√£o ‚Üí Notifica√ß√£o Fiscal):\")\n",
    "    print(f\"  Total de casos com NF: {len(tempos_analysis):,}\")\n",
    "    print(f\"  M√©dia: {tempos_analysis['dias_infracao_ate_nf'].mean():.0f} dias\")\n",
    "    print(f\"  Mediana: {tempos_analysis['dias_infracao_ate_nf'].median():.0f} dias\")\n",
    "    print(f\"  Desvio Padr√£o: {tempos_analysis['dias_infracao_ate_nf'].std():.0f} dias\")\n",
    "    print(f\"  M√≠nimo: {tempos_analysis['dias_infracao_ate_nf'].min():.0f} dias\")\n",
    "    print(f\"  M√°ximo: {tempos_analysis['dias_infracao_ate_nf'].max():.0f} dias\")\n",
    "    \n",
    "    # Percentis\n",
    "    print(f\"\\nüìä DISTRIBUI√á√ÉO POR PERCENTIS:\")\n",
    "    percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "    for p in percentiles:\n",
    "        valor = np.percentile(tempos_analysis['dias_infracao_ate_nf'], p)\n",
    "        print(f\"  P{p}: {valor:.0f} dias\")\n",
    "    \n",
    "    # Definir faixas de SLA\n",
    "    tempos_analysis['faixa_sla'] = pd.cut(\n",
    "        tempos_analysis['dias_infracao_ate_nf'],\n",
    "        bins=[0, 30, 60, 90, 180, 365, np.inf],\n",
    "        labels=['‚â§ 30 dias (Excelente)', '31-60 dias (√ìtimo)', \n",
    "                '61-90 dias (Bom)', '91-180 dias (Regular)',\n",
    "                '181-365 dias (Lento)', '> 365 dias (Cr√≠tico)']\n",
    "    )\n",
    "    \n",
    "    sla_dist = tempos_analysis['faixa_sla'].value_counts().sort_index()\n",
    "    sla_pct = (sla_dist / len(tempos_analysis) * 100).round(2)\n",
    "    \n",
    "    print(\"\\nüìä DISTRIBUI√á√ÉO POR FAIXA DE SLA:\")\n",
    "    for faixa, qtd, pct in zip(sla_dist.index, sla_dist.values, sla_pct.values):\n",
    "        print(f\"  {faixa}: {qtd:,} ({pct}%)\")\n",
    "    \n",
    "    # Visualiza√ß√£o - Distribui√ß√£o de Tempos\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Histograma - Dias at√© Notifica√ß√£o',\n",
    "            'Distribui√ß√£o por Faixa de SLA',\n",
    "            'Box Plot por Ano',\n",
    "            'Evolu√ß√£o da M√©dia ao Longo do Tempo'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "            [{\"type\": \"box\"}, {\"type\": \"scatter\"}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 1. Histograma\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=tempos_analysis['dias_infracao_ate_nf'],\n",
    "            nbinsx=100,\n",
    "            name='Dias at√© NF',\n",
    "            marker_color='steelblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Distribui√ß√£o SLA\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=sla_dist.values,\n",
    "            y=sla_dist.index.astype(str),\n",
    "            orientation='h',\n",
    "            text=sla_pct.values,\n",
    "            texttemplate='%{text}%',\n",
    "            textposition='auto',\n",
    "            marker_color=['green', 'lightgreen', 'yellow', 'orange', 'red', 'darkred']\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Box plot por ano\n",
    "    for ano in sorted(tempos_analysis['ano_infracao'].unique()):\n",
    "        dados_ano = tempos_analysis[tempos_analysis['ano_infracao'] == ano]['dias_infracao_ate_nf']\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=dados_ano,\n",
    "                name=str(ano),\n",
    "                boxmean='sd'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 4. Evolu√ß√£o temporal\n",
    "    tempo_por_ano = tempos_analysis.groupby('ano_infracao')['dias_infracao_ate_nf'].agg(['mean', 'median']).reset_index()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=tempo_por_ano['ano_infracao'],\n",
    "            y=tempo_por_ano['mean'],\n",
    "            mode='lines+markers',\n",
    "            name='M√©dia',\n",
    "            line=dict(color='blue', width=3)\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=tempo_por_ano['ano_infracao'],\n",
    "            y=tempo_por_ano['median'],\n",
    "            mode='lines+markers',\n",
    "            name='Mediana',\n",
    "            line=dict(color='red', width=3, dash='dash')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Dias\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Ano\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Dias\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Dias\", row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        showlegend=True,\n",
    "        title_text=\"‚è±Ô∏è An√°lise Completa de Tempestividade\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # An√°lise por regime tribut√°rio\n",
    "    print(\"\\nüèõÔ∏è TEMPESTIVIDADE POR REGIME TRIBUT√ÅRIO:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    tempo_por_regime = tempos_analysis.groupby('regime_tributario')['dias_infracao_ate_nf'].agg([\n",
    "        'count', 'mean', 'median', 'std'\n",
    "    ]).round(0)\n",
    "    tempo_por_regime.columns = ['Quantidade', 'M√©dia (dias)', 'Mediana (dias)', 'Desvio Padr√£o']\n",
    "    tempo_por_regime = tempo_por_regime.sort_values('M√©dia (dias)', ascending=False)\n",
    "    \n",
    "    print(tempo_por_regime.head(10))\n",
    "    \n",
    "    # An√°lise por faixa de valor\n",
    "    print(\"\\nüí∞ TEMPESTIVIDADE POR FAIXA DE VALOR:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    tempos_analysis['faixa_valor'] = pd.cut(\n",
    "        tempos_analysis['valor_total_infracao'],\n",
    "        bins=[0, 50000, 100000, 500000, 1000000, np.inf],\n",
    "        labels=['< 50k', '50k-100k', '100k-500k', '500k-1M', '> 1M']\n",
    "    )\n",
    "    \n",
    "    tempo_por_valor = tempos_analysis.groupby('faixa_valor')['dias_infracao_ate_nf'].agg([\n",
    "        'count', 'mean', 'median'\n",
    "    ]).round(0)\n",
    "    tempo_por_valor.columns = ['Quantidade', 'M√©dia (dias)', 'Mediana (dias)']\n",
    "    \n",
    "    print(tempo_por_valor)\n",
    "    \n",
    "    # Visualiza√ß√£o por valor\n",
    "    fig2 = px.box(\n",
    "        tempos_analysis,\n",
    "        x='faixa_valor',\n",
    "        y='dias_infracao_ate_nf',\n",
    "        title='Distribui√ß√£o de Tempo por Faixa de Valor da Infra√ß√£o',\n",
    "        labels={'dias_infracao_ate_nf': 'Dias at√© Notifica√ß√£o', 'faixa_valor': 'Faixa de Valor'},\n",
    "        color='faixa_valor',\n",
    "        color_discrete_sequence=px.colors.sequential.Reds\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(height=500, showlegend=False)\n",
    "    fig2.show()\n",
    "    \n",
    "    # Identificar casos cr√≠ticos\n",
    "    print(\"\\nüö® CASOS CR√çTICOS (> 365 DIAS):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    casos_criticos = tempos_analysis[tempos_analysis['dias_infracao_ate_nf'] > 365].copy()\n",
    "    casos_criticos = casos_criticos.sort_values('dias_infracao_ate_nf', ascending=False)\n",
    "    \n",
    "    print(f\"Total de casos cr√≠ticos: {len(casos_criticos):,}\")\n",
    "    print(f\"Percentual do total: {len(casos_criticos)/len(tempos_analysis)*100:.2f}%\")\n",
    "    print(f\"Valor total envolvido: {format_currency(casos_criticos['valor_total_infracao'].sum())}\")\n",
    "    \n",
    "    if len(casos_criticos) > 0:\n",
    "        print(\"\\nTop 10 casos mais demorados:\")\n",
    "        print(casos_criticos[['cnpj', 'nm_razao_social', 'valor_total_infracao', \n",
    "                              'dias_infracao_ate_nf', 'ano_infracao']].head(10))\n",
    "    \n",
    "    # Meta de SLA sugerida\n",
    "    print(\"\\nüéØ SUGEST√ÉO DE METAS DE SLA:\")\n",
    "    print(\"=\" * 80)\n",
    "    p75 = np.percentile(tempos_analysis['dias_infracao_ate_nf'], 75)\n",
    "    p90 = np.percentile(tempos_analysis['dias_infracao_ate_nf'], 90)\n",
    "    \n",
    "    print(f\"  Meta Bronze (75% dos casos): at√© {p75:.0f} dias\")\n",
    "    print(f\"  Meta Prata (90% dos casos): at√© {p90:.0f} dias\")\n",
    "    print(f\"  Meta Ouro (ideal): at√© 90 dias\")\n",
    "    print(f\"  Meta Platina (excel√™ncia): at√© 60 dias\")\n",
    "    \n",
    "    casos_dentro_90 = (tempos_analysis['dias_infracao_ate_nf'] <= 90).sum()\n",
    "    casos_dentro_60 = (tempos_analysis['dias_infracao_ate_nf'] <= 60).sum()\n",
    "    \n",
    "    print(f\"\\n  Casos dentro da Meta Ouro (90 dias): {casos_dentro_90:,} ({casos_dentro_90/len(tempos_analysis)*100:.2f}%)\")\n",
    "    print(f\"  Casos dentro da Meta Platina (60 dias): {casos_dentro_60:,} ({casos_dentro_60/len(tempos_analysis)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de tempestividade conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados insuficientes para an√°lise de tempestividade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960bc6d-f835-4903-a3dd-6c6d61cf9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 18: DASHBOARD INTERATIVO GERAL (COMPLETA)\n",
    "# ============================================================================\n",
    "\n",
    "if not df_fisc.empty and not df_dashboard.empty:\n",
    "    \n",
    "    print(\"üìä CRIANDO DASHBOARD INTERATIVO\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Preparar dados agregados\n",
    "    ano_mais_recente = df_fisc['ano_infracao'].max()\n",
    "    df_recente = df_fisc[df_fisc['ano_infracao'] == ano_mais_recente].copy()\n",
    "    \n",
    "    # Cards de m√©tricas principais\n",
    "    total_infracoes = len(df_recente)\n",
    "    total_nfs = df_recente['gerou_notificacao'].sum()\n",
    "    valor_total = df_recente['valor_total_infracao'].sum()\n",
    "    valor_nfs = df_recente['valor_total_nf'].sum()\n",
    "    taxa_conversao = (total_nfs / total_infracoes * 100) if total_infracoes > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìÖ ANO: {ano_mais_recente}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total de Infra√ß√µes: {total_infracoes:,}\")\n",
    "    print(f\"Total de NFs: {total_nfs:,}\")\n",
    "    print(f\"Valor Total Infra√ß√µes: {format_currency(valor_total)}\")\n",
    "    print(f\"Valor Total NFs: {format_currency(valor_nfs)}\")\n",
    "    print(f\"Taxa de Convers√£o: {taxa_conversao:.2f}%\")\n",
    "    \n",
    "    # Dashboard com m√∫ltiplos gr√°ficos\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Distribui√ß√£o por Regime Tribut√°rio',\n",
    "            'Top 10 Setores (CNAE)',\n",
    "            'Situa√ß√£o Final das Fiscaliza√ß√µes',\n",
    "            'Top 10 Munic√≠pios',\n",
    "            'Valores: Infra√ß√£o vs NF',\n",
    "            'Taxa de Convers√£o por M√™s',\n",
    "            'Distribui√ß√£o de Valores (Histograma)',\n",
    "            'Defesa Pr√©via e Pagamentos',\n",
    "            'Score de Efetividade'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "            [{\"type\": \"bar\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "            [{\"type\": \"histogram\"}, {\"type\": \"bar\"}, {\"type\": \"box\"}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 1. Distribui√ß√£o por Regime\n",
    "    regime_dist = df_recente['regime_tributario'].value_counts().head(5)\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=regime_dist.index, values=regime_dist.values, name='Regime'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Top 10 Setores\n",
    "    setor_valores = df_recente.groupby('cnae_secao_descricao')['valor_total_nf'].sum().nlargest(10)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=setor_valores.values/1_000_000, y=setor_valores.index, orientation='h',\n",
    "               marker_color='teal', name='Setores'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Situa√ß√£o Final\n",
    "    situacao_dist = df_recente['situacao_final'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=situacao_dist.index, values=situacao_dist.values, name='Situa√ß√£o'),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    # 4. Top 10 Munic√≠pios\n",
    "    municipio_count = df_recente['municipio'].value_counts().head(10)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=municipio_count.index, y=municipio_count.values,\n",
    "               marker_color='steelblue', name='Munic√≠pios'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 5. Valores Infra√ß√£o vs NF (scatter) - CORRIGIDO\n",
    "    df_with_nf = df_recente[df_recente['gerou_notificacao'] == 1].copy()\n",
    "    n_sample_dash = int(np.minimum(1000, len(df_with_nf)))\n",
    "    sample_data = df_with_nf.sample(n=n_sample_dash, random_state=42)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sample_data['valor_total_infracao'], y=sample_data['valor_total_nf'],\n",
    "                   mode='markers', marker=dict(size=5, color='red', opacity=0.5),\n",
    "                   name='Infra√ß√£o vs NF'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 6. Taxa de Convers√£o por M√™s - Criar campo m√™s de forma segura\n",
    "    df_recente_copy = df_recente.copy()\n",
    "    # Distribuir uniformemente pelos meses (aproxima√ß√£o)\n",
    "    np.random.seed(42)\n",
    "    df_recente_copy['mes'] = np.random.randint(1, 13, size=len(df_recente_copy))\n",
    "    \n",
    "    conversao_mes = df_recente_copy.groupby('mes').agg({\n",
    "        'cnpj': 'count',\n",
    "        'gerou_notificacao': 'sum'\n",
    "    }).reset_index()\n",
    "    conversao_mes.columns = ['mes', 'total', 'notificacoes']\n",
    "    conversao_mes['taxa'] = (conversao_mes['notificacoes'] / conversao_mes['total'] * 100)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=conversao_mes['mes'], y=conversao_mes['taxa'],\n",
    "                   mode='lines+markers', line=dict(color='green', width=3),\n",
    "                   name='Taxa Convers√£o'),\n",
    "        row=2, col=3\n",
    "    )\n",
    "    \n",
    "    # 7. Histograma de Valores\n",
    "    valores_positivos = df_recente['valor_total_infracao'][df_recente['valor_total_infracao'] > 0]\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=np.log10(valores_positivos),\n",
    "                     nbinsx=50, marker_color='purple', name='Log Valores'),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 8. Defesa Pr√©via\n",
    "    dp_stats = pd.DataFrame({\n",
    "        'Categoria': ['Com Pagamento DP', 'Sem Pagamento DP'],\n",
    "        'Quantidade': [\n",
    "            df_recente['pagou_dp_infracao'].sum(), \n",
    "            (df_recente['pagou_dp_infracao'] == 0).sum()\n",
    "        ]\n",
    "    })\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=dp_stats['Categoria'], y=dp_stats['Quantidade'],\n",
    "               marker_color=['lightgreen', 'lightcoral'], name='DP'),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # 9. Score de Efetividade (se dispon√≠vel no df_scores)\n",
    "    if not df_scores.empty:\n",
    "        df_scores_recente = df_scores[df_scores['ano_infracao'] == ano_mais_recente]\n",
    "        if len(df_scores_recente) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Box(y=df_scores_recente['score_efetividade_final'], name='Score',\n",
    "                       marker_color='gold'),\n",
    "                row=3, col=3\n",
    "            )\n",
    "    \n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        showlegend=False,\n",
    "        title_text=f\"üìä DASHBOARD EXECUTIVO FISCA - ANO {ano_mais_recente}\",\n",
    "        title_font_size=20\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Dashboard interativo criado!\")\n",
    "    print(\"\\nüí° DICA: Os gr√°ficos s√£o interativos! Voc√™ pode:\")\n",
    "    print(\"  - Fazer zoom\")\n",
    "    print(\"  - Selecionar √°reas\")\n",
    "    print(\"  - Ocultar/mostrar s√©ries\")\n",
    "    print(\"  - Exportar como imagem\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados insuficientes para dashboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de152a-aca1-4cb5-b97b-0f08f8ef4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 19: XGBOOST REGRESSIVO - PREVIS√ÉO DE VALORES DE NF (COMPLETA)\n",
    "# ============================================================================\n",
    "\n",
    "if ml_data is not None and len(ml_data['X']) >= 100:\n",
    "    \n",
    "    print(\"üí∞ XGBOOST REGRESSIVO - PREVIS√ÉO DE VALORES DE NOTIFICA√á√ÉO FISCAL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_ml = ml_data['df_ml']\n",
    "    \n",
    "    # Filtrar apenas casos com NF emitida\n",
    "    df_ml_reg = df_ml[df_ml['gerou_notificacao'] == 1].copy()\n",
    "    df_ml_reg = df_ml_reg[df_ml_reg['valor_total_nf'] > 0].copy()\n",
    "    \n",
    "    print(f\"\\nüìä Dataset para regress√£o:\")\n",
    "    print(f\"  Amostras: {len(df_ml_reg):,}\")\n",
    "    \n",
    "    if len(df_ml_reg) < 100:\n",
    "        print(\"‚ö†Ô∏è Dados insuficientes para regress√£o (m√≠nimo 100 casos)\")\n",
    "    else:\n",
    "        # Features para regress√£o\n",
    "        feature_cols_reg = [\n",
    "            'valor_log', 'valor_imposto_infracao', 'valor_multa_infracao',\n",
    "            'tem_multa', 'perc_multa', 'regime_encoded', 'cnae_encoded',\n",
    "            'municipio_encoded', 'uf_encoded', 'ano_infracao',\n",
    "            'dias_infracao_ate_nf'\n",
    "        ]\n",
    "        \n",
    "        # Preparar dados\n",
    "        X_reg = df_ml_reg[feature_cols_reg].fillna(0)\n",
    "        y_reg = df_ml_reg['valor_total_nf']\n",
    "        \n",
    "        # Log transform do target para melhor performance\n",
    "        y_reg_log = np.log1p(y_reg)\n",
    "        \n",
    "        print(f\"\\nüìä Estat√≠sticas do Target (Valor NF):\")\n",
    "        print(f\"  M√©dia: {format_currency(y_reg.mean())}\")\n",
    "        print(f\"  Mediana: {format_currency(y_reg.median())}\")\n",
    "        print(f\"  Desvio Padr√£o: {format_currency(y_reg.std())}\")\n",
    "        print(f\"  M√≠nimo: {format_currency(y_reg.min())}\")\n",
    "        print(f\"  M√°ximo: {format_currency(y_reg.max())}\")\n",
    "        \n",
    "        # Split train/test\n",
    "        X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "            X_reg, y_reg_log, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä Distribui√ß√£o dos dados:\")\n",
    "        print(f\"  Treino: {len(X_train_reg):,} amostras\")\n",
    "        print(f\"  Teste: {len(X_test_reg):,} amostras\")\n",
    "        \n",
    "        # Treinar XGBoost Regressor\n",
    "        print(\"\\nüîÑ Treinando XGBoost Regressor...\")\n",
    "        \n",
    "        xgb_reg_model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            objective='reg:squarederror'\n",
    "        )\n",
    "        \n",
    "        xgb_reg_model.fit(\n",
    "            X_train_reg, y_train_reg,\n",
    "            eval_set=[(X_test_reg, y_test_reg)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        y_pred_train = np.expm1(xgb_reg_model.predict(X_train_reg))\n",
    "        y_pred_test = np.expm1(xgb_reg_model.predict(X_test_reg))\n",
    "        y_test_original = np.expm1(y_test_reg)\n",
    "        y_train_original = np.expm1(y_train_reg)\n",
    "        \n",
    "        # M√©tricas\n",
    "        r2_train = r2_score(y_train_original, y_pred_train)\n",
    "        r2_test = r2_score(y_test_original, y_pred_test)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_original, y_pred_train))\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test_original, y_pred_test))\n",
    "        \n",
    "        # MAPE (Mean Absolute Percentage Error)\n",
    "        mape_train = np.mean(np.abs((y_train_original - y_pred_train) / y_train_original)) * 100\n",
    "        mape_test = np.mean(np.abs((y_test_original - y_pred_test) / y_test_original)) * 100\n",
    "        \n",
    "        print(\"\\nüìä RESULTADOS DO MODELO REGRESSIVO:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nTREINO:\")\n",
    "        print(f\"  R¬≤ Score: {r2_train:.4f}\")\n",
    "        print(f\"  RMSE: {format_currency(rmse_train)}\")\n",
    "        print(f\"  MAPE: {mape_train:.2f}%\")\n",
    "        \n",
    "        print(f\"\\nTESTE:\")\n",
    "        print(f\"  R¬≤ Score: {r2_test:.4f}\")\n",
    "        print(f\"  RMSE: {format_currency(rmse_test)}\")\n",
    "        print(f\"  MAPE: {mape_test:.2f}%\")\n",
    "        \n",
    "        # Import√¢ncia das features\n",
    "        feature_importance_reg = pd.DataFrame({\n",
    "            'feature': feature_cols_reg,\n",
    "            'importance': xgb_reg_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nüîù Top 10 Features Mais Importantes (Regress√£o):\")\n",
    "        print(feature_importance_reg.head(10))\n",
    "        \n",
    "        # Visualiza√ß√£o - Feature Importance\n",
    "        fig1 = px.bar(\n",
    "            feature_importance_reg.head(12),\n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            orientation='h',\n",
    "            title='Import√¢ncia das Features - XGBoost Regressor',\n",
    "            labels={'importance': 'Import√¢ncia', 'feature': 'Feature'},\n",
    "            color='importance',\n",
    "            color_continuous_scale='Viridis'\n",
    "        )\n",
    "        fig1.update_layout(height=500)\n",
    "        fig1.show()\n",
    "        \n",
    "        # Gr√°fico: Predito vs Real\n",
    "        fig2 = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=('Treino: Predito vs Real', 'Teste: Predito vs Real')\n",
    "        )\n",
    "        \n",
    "        # Treino\n",
    "        fig2.add_trace(\n",
    "            go.Scatter(\n",
    "                x=y_train_original,\n",
    "                y=y_pred_train,\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, color='blue', opacity=0.5),\n",
    "                name='Treino'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Linha diagonal perfeita - CORRIGIDO\n",
    "        max_val_train = float(np.maximum(y_train_original.max(), y_pred_train.max()))\n",
    "        fig2.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[0, max_val_train],\n",
    "                y=[0, max_val_train],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                name='Perfeito',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Teste\n",
    "        fig2.add_trace(\n",
    "            go.Scatter(\n",
    "                x=y_test_original,\n",
    "                y=y_pred_test,\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, color='green', opacity=0.5),\n",
    "                name='Teste'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Linha diagonal perfeita - CORRIGIDO\n",
    "        max_val_test = float(np.maximum(y_test_original.max(), y_pred_test.max()))\n",
    "        fig2.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[0, max_val_test],\n",
    "                y=[0, max_val_test],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                name='Perfeito',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig2.update_xaxes(title_text=\"Valor Real (R$)\", type=\"log\", row=1, col=1)\n",
    "        fig2.update_xaxes(title_text=\"Valor Real (R$)\", type=\"log\", row=1, col=2)\n",
    "        fig2.update_yaxes(title_text=\"Valor Predito (R$)\", type=\"log\", row=1, col=1)\n",
    "        fig2.update_yaxes(title_text=\"Valor Predito (R$)\", type=\"log\", row=1, col=2)\n",
    "        \n",
    "        fig2.update_layout(\n",
    "            height=500,\n",
    "            title_text=\"Compara√ß√£o: Valores Preditos vs Reais\"\n",
    "        )\n",
    "        fig2.show()\n",
    "        \n",
    "        # An√°lise de Res√≠duos\n",
    "        residuos_test = y_test_original - y_pred_test\n",
    "        residuos_pct = (residuos_test / y_test_original) * 100\n",
    "        \n",
    "        fig3 = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=('Distribui√ß√£o dos Res√≠duos (%)', 'Res√≠duos vs Valores Preditos')\n",
    "        )\n",
    "        \n",
    "        # Histograma de res√≠duos percentuais\n",
    "        fig3.add_trace(\n",
    "            go.Histogram(\n",
    "                x=residuos_pct,\n",
    "                nbinsx=50,\n",
    "                marker_color='coral',\n",
    "                name='Res√≠duos %'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Res√≠duos vs Preditos\n",
    "        fig3.add_trace(\n",
    "            go.Scatter(\n",
    "                x=y_pred_test,\n",
    "                y=residuos_test,\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, color='purple', opacity=0.5),\n",
    "                name='Res√≠duos'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Linha zero\n",
    "        fig3.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[float(y_pred_test.min()), float(y_pred_test.max())],\n",
    "                y=[0, 0],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig3.update_xaxes(title_text=\"Erro Percentual (%)\", row=1, col=1)\n",
    "        fig3.update_xaxes(title_text=\"Valor Predito (R$)\", type=\"log\", row=1, col=2)\n",
    "        fig3.update_yaxes(title_text=\"Frequ√™ncia\", row=1, col=1)\n",
    "        fig3.update_yaxes(title_text=\"Res√≠duo (R$)\", row=1, col=2)\n",
    "        \n",
    "        fig3.update_layout(\n",
    "            height=500,\n",
    "            showlegend=False,\n",
    "            title_text=\"An√°lise de Res√≠duos\"\n",
    "        )\n",
    "        fig3.show()\n",
    "        \n",
    "        # An√°lise por faixa de valor\n",
    "        print(\"\\nüìä PERFORMANCE POR FAIXA DE VALOR:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        df_ml_reg['valor_predito'] = np.expm1(xgb_reg_model.predict(X_reg))\n",
    "        df_ml_reg['faixa_valor_nf'] = pd.cut(\n",
    "            df_ml_reg['valor_total_nf'],\n",
    "            bins=[0, 50000, 100000, 500000, 1000000, np.inf],\n",
    "            labels=['< 50k', '50k-100k', '100k-500k', '500k-1M', '> 1M']\n",
    "        )\n",
    "        \n",
    "        performance_faixa = df_ml_reg.groupby('faixa_valor_nf').apply(\n",
    "            lambda x: pd.Series({\n",
    "                'quantidade': len(x),\n",
    "                'r2': r2_score(x['valor_total_nf'], x['valor_predito']),\n",
    "                'mape': np.mean(np.abs((x['valor_total_nf'] - x['valor_predito']) / x['valor_total_nf'])) * 100,\n",
    "                'valor_medio_real': x['valor_total_nf'].mean(),\n",
    "                'valor_medio_pred': x['valor_predito'].mean()\n",
    "            })\n",
    "        ).reset_index()\n",
    "        \n",
    "        print(performance_faixa)\n",
    "        \n",
    "        # Visualiza√ß√£o por faixa\n",
    "        fig4 = go.Figure()\n",
    "        \n",
    "        fig4.add_trace(go.Bar(\n",
    "            x=performance_faixa['faixa_valor_nf'],\n",
    "            y=performance_faixa['r2'],\n",
    "            name='R¬≤ por Faixa',\n",
    "            marker_color='teal',\n",
    "            text=performance_faixa['r2'].round(3),\n",
    "            textposition='auto'\n",
    "        ))\n",
    "        \n",
    "        fig4.update_layout(\n",
    "            title='R¬≤ Score por Faixa de Valor',\n",
    "            xaxis_title='Faixa de Valor',\n",
    "            yaxis_title='R¬≤ Score',\n",
    "            height=400\n",
    "        )\n",
    "        fig4.show()\n",
    "        \n",
    "        # Salvar modelo\n",
    "        xgb_reg_results = {\n",
    "            'model': xgb_reg_model,\n",
    "            'feature_importance': feature_importance_reg,\n",
    "            'r2_train': r2_train,\n",
    "            'r2_test': r2_test,\n",
    "            'rmse_test': rmse_test,\n",
    "            'mape_test': mape_test,\n",
    "            'predictions': y_pred_test\n",
    "        }\n",
    "        \n",
    "        print(\"\\n‚úÖ XGBoost Regressivo treinado com sucesso!\")\n",
    "        print(f\"\\nüí° INTERPRETA√á√ÉO:\")\n",
    "        print(f\"  - O modelo explica {r2_test*100:.2f}% da varia√ß√£o dos valores de NF\")\n",
    "        print(f\"  - Erro m√©dio percentual: {mape_test:.2f}%\")\n",
    "        print(f\"  - Pode ser usado para estimar valores esperados de NF\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados ML n√£o dispon√≠veis ou insuficientes para regress√£o\")\n",
    "    xgb_reg_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6b7b8-5a77-4c59-8a43-df1b82402283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 20: CLUSTERING HIER√ÅRQUICO (DENDROGRAMA) - CORRIGIDA\n",
    "# ============================================================================\n",
    "\n",
    "if cluster_results is not None:\n",
    "    \n",
    "    print(\"üå≥ AN√ÅLISE DE CLUSTERING HIER√ÅRQUICO\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "    from scipy.spatial.distance import pdist\n",
    "    \n",
    "    df_ml = cluster_results['df_clustered']\n",
    "    X_cluster_scaled = cluster_results['scaler'].transform(\n",
    "        df_ml[[\n",
    "            'valor_log', 'valor_imposto_infracao', 'valor_multa_infracao',\n",
    "            'perc_multa', 'regime_encoded', 'cnae_encoded',\n",
    "            'dias_infracao_ate_nf', 'ano_infracao'\n",
    "        ]].fillna(0)\n",
    "    )\n",
    "    \n",
    "    # Usar amostra para dendrograma (computacionalmente intensivo) - CORRIGIDO\n",
    "    sample_size = int(np.minimum(1000, len(X_cluster_scaled)))\n",
    "    X_sample = X_cluster_scaled[:sample_size]\n",
    "    \n",
    "    print(f\"\\nüìä Gerando dendrograma com {sample_size} amostras...\")\n",
    "    \n",
    "    # Calcular linkage\n",
    "    linkage_matrix = linkage(X_sample, method='ward')\n",
    "    \n",
    "    # Visualiza√ß√£o do dendrograma\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Preparar dados do dendrograma\n",
    "    dend = dendrogram(linkage_matrix, no_plot=True)\n",
    "    \n",
    "    # Criar dendrograma interativo\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i in range(len(dend['icoord'])):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dend['icoord'][i],\n",
    "            y=dend['dcoord'][i],\n",
    "            mode='lines',\n",
    "            line=dict(color='black', width=1),\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Dendrograma - Clustering Hier√°rquico (Ward)',\n",
    "        xaxis_title='√çndice da Amostra',\n",
    "        yaxis_title='Dist√¢ncia',\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # An√°lise de silhueta por cluster\n",
    "    print(\"\\nüìä AN√ÅLISE DE SILHUETA POR CLUSTER:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    from sklearn.metrics import silhouette_samples\n",
    "    \n",
    "    # Calcular silhouette com amostra se dataset muito grande\n",
    "    if len(X_cluster_scaled) > 10000:\n",
    "        print(f\"‚ö° Calculando silhuette em amostra de 10.000 registros...\")\n",
    "        sample_indices = np.random.choice(len(X_cluster_scaled), 10000, replace=False)\n",
    "        X_silhouette = X_cluster_scaled[sample_indices]\n",
    "        clusters_silhouette = df_ml['cluster'].iloc[sample_indices]\n",
    "    else:\n",
    "        X_silhouette = X_cluster_scaled\n",
    "        clusters_silhouette = df_ml['cluster']\n",
    "    \n",
    "    silhouette_vals = silhouette_samples(X_silhouette, clusters_silhouette)\n",
    "    \n",
    "    # Criar DataFrame tempor√°rio para an√°lise\n",
    "    df_silhouette_temp = pd.DataFrame({\n",
    "        'cluster': clusters_silhouette,\n",
    "        'silhouette': silhouette_vals\n",
    "    })\n",
    "    \n",
    "    silhueta_por_cluster = df_silhouette_temp.groupby('cluster')['silhouette'].agg([\n",
    "        'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(3)\n",
    "    \n",
    "    print(silhueta_por_cluster)\n",
    "    \n",
    "    # Visualiza√ß√£o da silhueta\n",
    "    fig2 = go.Figure()\n",
    "    \n",
    "    y_lower = 0\n",
    "    for cluster_id in sorted(df_silhouette_temp['cluster'].unique()):\n",
    "        cluster_silhouette = df_silhouette_temp[df_silhouette_temp['cluster'] == cluster_id]['silhouette'].values\n",
    "        cluster_silhouette.sort()\n",
    "        \n",
    "        y_upper = y_lower + len(cluster_silhouette)\n",
    "        \n",
    "        fig2.add_trace(go.Bar(\n",
    "            x=cluster_silhouette,\n",
    "            y=list(range(y_lower, y_upper)),\n",
    "            orientation='h',\n",
    "            name=f'Cluster {cluster_id}',\n",
    "            showlegend=True\n",
    "        ))\n",
    "        \n",
    "        y_lower = y_upper\n",
    "    \n",
    "    # Linha de silhueta m√©dia\n",
    "    avg_silhouette = silhouette_vals.mean()\n",
    "    fig2.add_vline(\n",
    "        x=avg_silhouette,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=f\"M√©dia: {avg_silhouette:.3f}\"\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title='An√°lise de Silhueta por Cluster',\n",
    "        xaxis_title='Coeficiente de Silhueta',\n",
    "        yaxis_title='√çndice da Amostra',\n",
    "        height=600,\n",
    "        barmode='overlay'\n",
    "    )\n",
    "    \n",
    "    fig2.show()\n",
    "    \n",
    "    # Compara√ß√£o de clusters no espa√ßo 3D (PCA)\n",
    "    print(\"\\nüîÑ Gerando visualiza√ß√£o 3D (PCA)...\")\n",
    "    \n",
    "    pca_3d = PCA(n_components=3, random_state=42)\n",
    "    X_pca_3d = pca_3d.fit_transform(X_cluster_scaled)\n",
    "    \n",
    "    df_ml['pca1_3d'] = X_pca_3d[:, 0]\n",
    "    df_ml['pca2_3d'] = X_pca_3d[:, 1]\n",
    "    df_ml['pca3_3d'] = X_pca_3d[:, 2]\n",
    "    \n",
    "    print(f\"üìä Vari√¢ncia explicada (3 componentes): {pca_3d.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "    \n",
    "    # Amostra para visualiza√ß√£o - CORRIGIDO\n",
    "    n_sample_3d = int(np.minimum(5000, len(df_ml)))\n",
    "    df_plot_3d = df_ml.sample(n=n_sample_3d, random_state=42)\n",
    "    \n",
    "    fig3 = px.scatter_3d(\n",
    "        df_plot_3d,\n",
    "        x='pca1_3d',\n",
    "        y='pca2_3d',\n",
    "        z='pca3_3d',\n",
    "        color='cluster',\n",
    "        hover_data=['valor_total_infracao', 'regime_tributario'],\n",
    "        title='Visualiza√ß√£o 3D dos Clusters (PCA)',\n",
    "        labels={\n",
    "            'pca1_3d': f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "            'pca2_3d': f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.1f}%)',\n",
    "            'pca3_3d': f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.1f}%)'\n",
    "        },\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    \n",
    "    fig3.update_traces(marker=dict(size=3, opacity=0.6))\n",
    "    fig3.update_layout(height=700)\n",
    "    fig3.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de clustering hier√°rquico conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Resultados de clustering n√£o dispon√≠veis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e9ec2-e626-4e3a-9d8a-0f6a392db628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 21: DETEC√á√ÉO DE OUTLIERS - ISOLATION FOREST (COMPLETA)\n",
    "# ============================================================================\n",
    "\n",
    "if ml_data is not None:\n",
    "    \n",
    "    print(\"üéØ DETEC√á√ÉO DE OUTLIERS - ISOLATION FOREST\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Identifica√ß√£o de casos fiscais an√¥malos/at√≠picos\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    df_ml = ml_data['df_ml']\n",
    "    \n",
    "    # Features para detec√ß√£o de outliers\n",
    "    outlier_features = [\n",
    "        'valor_log', 'valor_imposto_infracao', 'valor_multa_infracao',\n",
    "        'perc_multa', 'dias_infracao_ate_nf'\n",
    "    ]\n",
    "    \n",
    "    X_outlier = df_ml[outlier_features].fillna(0)\n",
    "    \n",
    "    # Normalizar\n",
    "    scaler_outlier = StandardScaler()\n",
    "    X_outlier_scaled = scaler_outlier.fit_transform(X_outlier)\n",
    "    \n",
    "    print(\"\\nüîÑ Treinando Isolation Forest...\")\n",
    "    \n",
    "    # Treinar Isolation Forest\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=0.05,  # 5% esperado de outliers\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    outlier_labels = iso_forest.fit_predict(X_outlier_scaled)\n",
    "    outlier_scores = iso_forest.score_samples(X_outlier_scaled)\n",
    "    \n",
    "    df_ml['outlier_label'] = outlier_labels\n",
    "    df_ml['outlier_score'] = outlier_scores\n",
    "    df_ml['is_outlier'] = (outlier_labels == -1).astype(int)\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    n_outliers = (outlier_labels == -1).sum()\n",
    "    n_normal = (outlier_labels == 1).sum()\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADOS DA DETEC√á√ÉO:\")\n",
    "    print(f\"  Total de casos: {len(df_ml):,}\")\n",
    "    print(f\"  Outliers detectados: {n_outliers:,} ({n_outliers/len(df_ml)*100:.2f}%)\")\n",
    "    print(f\"  Casos normais: {n_normal:,} ({n_normal/len(df_ml)*100:.2f}%)\")\n",
    "    \n",
    "    # Caracter√≠sticas dos outliers\n",
    "    df_outliers = df_ml[df_ml['is_outlier'] == 1].copy()\n",
    "    df_normal = df_ml[df_ml['is_outlier'] == 0].copy()\n",
    "    \n",
    "    print(\"\\nüìä COMPARA√á√ÉO: OUTLIERS vs NORMAIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparacao = pd.DataFrame({\n",
    "        'M√©trica': [\n",
    "            'Quantidade',\n",
    "            'Valor M√©dio Infra√ß√£o',\n",
    "            'Valor M√©dio NF',\n",
    "            'Taxa Notifica√ß√£o (%)',\n",
    "            'M√©dia Dias at√© NF',\n",
    "            'Taxa Ciclo Completo (%)'\n",
    "        ],\n",
    "        'Outliers': [\n",
    "            len(df_outliers),\n",
    "            df_outliers['valor_total_infracao'].mean(),\n",
    "            df_outliers['valor_total_nf'].mean(),\n",
    "            df_outliers['gerou_notificacao'].mean() * 100,\n",
    "            df_outliers['dias_infracao_ate_nf'].mean(),\n",
    "            df_outliers['ciclo_completo'].mean() * 100\n",
    "        ],\n",
    "        'Normais': [\n",
    "            len(df_normal),\n",
    "            df_normal['valor_total_infracao'].mean(),\n",
    "            df_normal['valor_total_nf'].mean(),\n",
    "            df_normal['gerou_notificacao'].mean() * 100,\n",
    "            df_normal['dias_infracao_ate_nf'].mean(),\n",
    "            df_normal['ciclo_completo'].mean() * 100\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(comparacao)\n",
    "    \n",
    "    # Visualiza√ß√£o - Distribui√ß√£o dos scores\n",
    "    fig1 = go.Figure()\n",
    "    \n",
    "    fig1.add_trace(go.Histogram(\n",
    "        x=df_normal['outlier_score'],\n",
    "        name='Normais',\n",
    "        marker_color='lightblue',\n",
    "        opacity=0.7,\n",
    "        nbinsx=50\n",
    "    ))\n",
    "    \n",
    "    fig1.add_trace(go.Histogram(\n",
    "        x=df_outliers['outlier_score'],\n",
    "        name='Outliers',\n",
    "        marker_color='red',\n",
    "        opacity=0.7,\n",
    "        nbinsx=50\n",
    "    ))\n",
    "    \n",
    "    fig1.update_layout(\n",
    "        title='Distribui√ß√£o dos Scores de Anomalia',\n",
    "        xaxis_title='Score de Anomalia (menor = mais an√¥malo)',\n",
    "        yaxis_title='Frequ√™ncia',\n",
    "        barmode='overlay',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig1.show()\n",
    "    \n",
    "    # Visualiza√ß√£o 2D - PCA\n",
    "    pca_outlier = PCA(n_components=2, random_state=42)\n",
    "    X_pca_outlier = pca_outlier.fit_transform(X_outlier_scaled)\n",
    "    \n",
    "    df_ml['pca1_outlier'] = X_pca_outlier[:, 0]\n",
    "    df_ml['pca2_outlier'] = X_pca_outlier[:, 1]\n",
    "    \n",
    "    # Amostra para visualiza√ß√£o - CORRIGIDO\n",
    "    n_sample_outlier = int(np.minimum(5000, len(df_ml)))\n",
    "    df_plot_outlier = df_ml.sample(n=n_sample_outlier, random_state=42)\n",
    "    \n",
    "    fig2 = px.scatter(\n",
    "        df_plot_outlier,\n",
    "        x='pca1_outlier',\n",
    "        y='pca2_outlier',\n",
    "        color='is_outlier',\n",
    "        hover_data=['cnpj', 'valor_total_infracao', 'dias_infracao_ate_nf'],\n",
    "        title='Visualiza√ß√£o de Outliers no Espa√ßo PCA',\n",
    "        labels={\n",
    "            'pca1_outlier': f'PC1 ({pca_outlier.explained_variance_ratio_[0]*100:.1f}%)',\n",
    "            'pca2_outlier': f'PC2 ({pca_outlier.explained_variance_ratio_[1]*100:.1f}%)',\n",
    "            'is_outlier': 'Tipo'\n",
    "        },\n",
    "        color_discrete_map={0: 'lightblue', 1: 'red'}\n",
    "    )\n",
    "    \n",
    "    fig2.update_traces(marker=dict(size=5, opacity=0.6))\n",
    "    fig2.update_layout(height=600)\n",
    "    fig2.show()\n",
    "    \n",
    "    # Top 20 outliers mais extremos\n",
    "    print(\"\\nüö® TOP 20 OUTLIERS MAIS EXTREMOS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    top_outliers = df_outliers.nsmallest(20, 'outlier_score')[\n",
    "        ['cnpj', 'nm_razao_social', 'valor_total_infracao', 'valor_total_nf',\n",
    "         'dias_infracao_ate_nf', 'outlier_score', 'regime_tributario']\n",
    "    ]\n",
    "    \n",
    "    print(top_outliers)\n",
    "    \n",
    "    # An√°lise por caracter√≠sticas\n",
    "    print(\"\\nüìä DISTRIBUI√á√ÉO DE OUTLIERS POR REGIME TRIBUT√ÅRIO:\")\n",
    "    outliers_regime = df_outliers['regime_tributario'].value_counts().head(10)\n",
    "    print(outliers_regime)\n",
    "    \n",
    "    print(\"\\nüìä DISTRIBUI√á√ÉO DE OUTLIERS POR SETOR (CNAE):\")\n",
    "    outliers_setor = df_outliers['cnae_secao_descricao'].value_counts().head(10)\n",
    "    print(outliers_setor)\n",
    "    \n",
    "    # Visualiza√ß√£o - Caracter√≠sticas dos outliers\n",
    "    fig3 = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Distribui√ß√£o de Valores (Outliers)',\n",
    "            'Distribui√ß√£o de Tempo at√© NF (Outliers)',\n",
    "            'Top 10 Regimes (Outliers)',\n",
    "            'Taxa de Notifica√ß√£o'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 1. Valores\n",
    "    valores_out_positivos = df_outliers['valor_total_infracao'][df_outliers['valor_total_infracao'] > 0]\n",
    "    fig3.add_trace(\n",
    "        go.Histogram(\n",
    "            x=np.log10(valores_out_positivos),\n",
    "            marker_color='red',\n",
    "            name='Valores',\n",
    "            nbinsx=30\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Tempo at√© NF\n",
    "    dias_out_positivos = df_outliers['dias_infracao_ate_nf'][df_outliers['dias_infracao_ate_nf'] > 0]\n",
    "    fig3.add_trace(\n",
    "        go.Histogram(\n",
    "            x=dias_out_positivos,\n",
    "            marker_color='orange',\n",
    "            name='Dias',\n",
    "            nbinsx=30\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Top Regimes\n",
    "    fig3.add_trace(\n",
    "        go.Bar(\n",
    "            y=outliers_regime.index[:10],\n",
    "            x=outliers_regime.values[:10],\n",
    "            orientation='h',\n",
    "            marker_color='purple',\n",
    "            name='Regimes'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Taxa de Notifica√ß√£o\n",
    "    taxa_notif = pd.DataFrame({\n",
    "        'Grupo': ['Outliers', 'Normais'],\n",
    "        'Taxa': [\n",
    "            df_outliers['gerou_notificacao'].mean() * 100,\n",
    "            df_normal['gerou_notificacao'].mean() * 100\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Bar(\n",
    "            x=taxa_notif['Grupo'],\n",
    "            y=taxa_notif['Taxa'],\n",
    "            marker_color=['red', 'lightblue'],\n",
    "            name='Taxa',\n",
    "            text=taxa_notif['Taxa'].round(1),\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig3.update_xaxes(title_text=\"Log10(Valor)\", row=1, col=1)\n",
    "    fig3.update_xaxes(title_text=\"Dias\", row=1, col=2)\n",
    "    fig3.update_xaxes(title_text=\"Quantidade\", row=2, col=1)\n",
    "    fig3.update_yaxes(title_text=\"Taxa (%)\", row=2, col=2)\n",
    "    \n",
    "    fig3.update_layout(\n",
    "        height=800,\n",
    "        showlegend=False,\n",
    "        title_text=\"üìä An√°lise Detalhada dos Outliers\"\n",
    "    )\n",
    "    \n",
    "    fig3.show()\n",
    "    \n",
    "    # Recomenda√ß√µes\n",
    "    print(\"\\nüí° RECOMENDA√á√ïES PARA CASOS OUTLIERS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"1. Casos com valores extremamente altos merecem aten√ß√£o especial\")\n",
    "    print(\"2. Outliers com tempo prolongado precisam de acompanhamento priorit√°rio\")\n",
    "    print(\"3. Padr√µes an√¥malos podem indicar:\")\n",
    "    print(\"   - Casos complexos que demandam mais tempo\")\n",
    "    print(\"   - Situa√ß√µes at√≠picas que fogem do padr√£o normal\")\n",
    "    print(\"   - Poss√≠veis gargalos no processo\")\n",
    "    print(\"   - Oportunidades de melhoria na triagem inicial\")\n",
    "    \n",
    "    # Salvar resultados\n",
    "    outlier_results = {\n",
    "        'model': iso_forest,\n",
    "        'scaler': scaler_outlier,\n",
    "        'df_outliers': df_outliers,\n",
    "        'n_outliers': n_outliers,\n",
    "        'outlier_rate': n_outliers/len(df_ml)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de outliers conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados ML n√£o dispon√≠veis\")\n",
    "    outlier_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf02bf-6be0-4d4f-b987-70dbe09f2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 22: AN√ÅLISE DE S√âRIES TEMPORAIS\n",
    "# ============================================================================\n",
    "\n",
    "if not df_dashboard.empty:\n",
    "    \n",
    "    print(\"üìà AN√ÅLISE DE S√âRIES TEMPORAIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Preparar dados temporais\n",
    "    ts_data = df_dashboard.sort_values('ano').copy()\n",
    "    ts_data['ano'] = pd.to_datetime(ts_data['ano'], format='%Y')\n",
    "    \n",
    "    print(f\"\\nüìä Per√≠odo de an√°lise: {ts_data['ano'].min().year} a {ts_data['ano'].max().year}\")\n",
    "    \n",
    "    # M√©tricas para an√°lise temporal\n",
    "    metricas = {\n",
    "        'qtd_infracoes_lavradas': 'Quantidade de Infra√ß√µes',\n",
    "        'qtd_nfs_emitidas': 'Quantidade de NFs',\n",
    "        'valor_total_infracoes': 'Valor Total Infra√ß√µes (R$)',\n",
    "        'valor_total_nfs': 'Valor Total NFs (R$)',\n",
    "        'taxa_conversao_infracao_nf': 'Taxa de Convers√£o (%)',\n",
    "        'media_dias_infracao_nf': 'M√©dia Dias at√© NF'\n",
    "    }\n",
    "    \n",
    "    # Calcular taxas de crescimento\n",
    "    print(\"\\nüìä TAXAS DE CRESCIMENTO ANUAL:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    crescimento = pd.DataFrame()\n",
    "    for col, nome in metricas.items():\n",
    "        if col in ts_data.columns:\n",
    "            valores = ts_data[col].values\n",
    "            if len(valores) > 1:\n",
    "                taxas = [(valores[i] - valores[i-1]) / valores[i-1] * 100 \n",
    "                        if valores[i-1] != 0 else 0 \n",
    "                        for i in range(1, len(valores))]\n",
    "                taxa_media = np.mean(taxas) if taxas else 0\n",
    "                print(f\"{nome}: {taxa_media:+.2f}% ao ano\")\n",
    "    \n",
    "    # Visualiza√ß√£o - M√∫ltiplas s√©ries temporais\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=tuple(metricas.values()),\n",
    "        vertical_spacing=0.1,\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    row_col_map = [(1,1), (1,2), (2,1), (2,2), (3,1), (3,2)]\n",
    "    \n",
    "    for idx, (col, nome) in enumerate(metricas.items()):\n",
    "        if col in ts_data.columns:\n",
    "            row, col_pos = row_col_map[idx]\n",
    "            \n",
    "            # S√©rie original\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=ts_data['ano'],\n",
    "                    y=ts_data[col],\n",
    "                    mode='lines+markers',\n",
    "                    name=nome,\n",
    "                    line=dict(width=3),\n",
    "                    marker=dict(size=8),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=row, col=col_pos\n",
    "            )\n",
    "            \n",
    "            # Linha de tend√™ncia (regress√£o linear simples)\n",
    "            if len(ts_data) >= 3:\n",
    "                from scipy import stats\n",
    "                x_numeric = np.arange(len(ts_data))\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                    x_numeric, ts_data[col].fillna(0)\n",
    "                )\n",
    "                y_trend = slope * x_numeric + intercept\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=ts_data['ano'],\n",
    "                        y=y_trend,\n",
    "                        mode='lines',\n",
    "                        line=dict(dash='dash', width=2, color='red'),\n",
    "                        name='Tend√™ncia',\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=row, col=col_pos\n",
    "                )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        title_text=\"üìà An√°lise de S√©ries Temporais - Principais M√©tricas\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Decomposi√ß√£o de tend√™ncia (se houver dados suficientes)\n",
    "    if len(ts_data) >= 4:\n",
    "        print(\"\\nüìä AN√ÅLISE DE TEND√äNCIA (Regress√£o Linear):\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        from scipy import stats\n",
    "        \n",
    "        for col, nome in metricas.items():\n",
    "            if col in ts_data.columns:\n",
    "                x_numeric = np.arange(len(ts_data))\n",
    "                y_values = ts_data[col].fillna(0).values\n",
    "                \n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(x_numeric, y_values)\n",
    "                \n",
    "                print(f\"\\n{nome}:\")\n",
    "                print(f\"  Inclina√ß√£o: {slope:+.2f}\")\n",
    "                print(f\"  R¬≤ (ajuste): {r_value**2:.4f}\")\n",
    "                print(f\"  p-value: {p_value:.4f}\")\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    if slope > 0:\n",
    "                        print(f\"  ‚úÖ Tend√™ncia de CRESCIMENTO estatisticamente significativa\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ö†Ô∏è Tend√™ncia de QUEDA estatisticamente significativa\")\n",
    "                else:\n",
    "                    print(f\"  ‚ÑπÔ∏è Sem tend√™ncia estatisticamente significativa\")\n",
    "    \n",
    "    # Varia√ß√£o acumulada\n",
    "    print(\"\\nüìä VARIA√á√ÉO ACUMULADA (Base: Primeiro Ano):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for col, nome in metricas.items():\n",
    "        if col in ts_data.columns:\n",
    "            valores = ts_data[col].values\n",
    "            if len(valores) > 1 and valores[0] != 0:\n",
    "                variacao_total = ((valores[-1] - valores[0]) / valores[0] * 100)\n",
    "                print(f\"{nome}: {variacao_total:+.2f}%\")\n",
    "    \n",
    "    # Previs√£o simples (m√©dia m√≥vel)\n",
    "    if len(ts_data) >= 3:\n",
    "        print(\"\\nüîÆ PROJE√á√ÉO PARA PR√ìXIMO ANO (M√©dia M√≥vel):\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        janela = min(3, len(ts_data))\n",
    "        \n",
    "        for col, nome in metricas.items():\n",
    "            if col in ts_data.columns:\n",
    "                ultimos_valores = ts_data[col].tail(janela).values\n",
    "                previsao = np.mean(ultimos_valores)\n",
    "                print(f\"{nome}: {previsao:.2f}\")\n",
    "    \n",
    "    # Sazonalidade mensal (se houver dados de fiscaliza√ß√µes)\n",
    "    if not df_fisc.empty and 'ano_infracao' in df_fisc.columns:\n",
    "        print(\"\\nüìä AN√ÅLISE DE SAZONALIDADE MENSAL:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Criar campo de m√™s\n",
    "        df_fisc_temp = df_fisc.copy()\n",
    "        \n",
    "        # Tentar extrair m√™s da data de infra√ß√£o se dispon√≠vel\n",
    "        if 'data_infracao' in df_fisc_temp.columns:\n",
    "            df_fisc_temp['mes'] = pd.to_datetime(df_fisc_temp['data_infracao'], errors='coerce').dt.month\n",
    "        else:\n",
    "            # Usar distribui√ß√£o uniforme como aproxima√ß√£o\n",
    "            np.random.seed(42)\n",
    "            df_fisc_temp['mes'] = np.random.randint(1, 13, size=len(df_fisc_temp))\n",
    "        \n",
    "        sazonalidade = df_fisc_temp.groupby('mes').agg({\n",
    "            'cnpj': 'count',\n",
    "            'valor_total_infracao': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        sazonalidade.columns = ['mes', 'quantidade', 'valor_total']\n",
    "        sazonalidade['mes_nome'] = sazonalidade['mes'].map({\n",
    "            1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Abr', 5: 'Mai', 6: 'Jun',\n",
    "            7: 'Jul', 8: 'Ago', 9: 'Set', 10: 'Out', 11: 'Nov', 12: 'Dez'\n",
    "        })\n",
    "        \n",
    "        # Normalizar para m√©dia = 100\n",
    "        media_qtd = sazonalidade['quantidade'].mean()\n",
    "        sazonalidade['indice_qtd'] = (sazonalidade['quantidade'] / media_qtd * 100).round(1)\n",
    "        \n",
    "        print(\"\\n√çndice de Sazonalidade (M√©dia = 100):\")\n",
    "        print(sazonalidade[['mes_nome', 'quantidade', 'indice_qtd']])\n",
    "        \n",
    "        # Visualiza√ß√£o sazonalidade\n",
    "        fig2 = go.Figure()\n",
    "        \n",
    "        fig2.add_trace(go.Bar(\n",
    "            x=sazonalidade['mes_nome'],\n",
    "            y=sazonalidade['indice_qtd'],\n",
    "            marker_color=np.where(sazonalidade['indice_qtd'] > 100, 'green', 'lightcoral'),\n",
    "            text=sazonalidade['indice_qtd'],\n",
    "            textposition='auto',\n",
    "            name='√çndice'\n",
    "        ))\n",
    "        \n",
    "        fig2.add_hline(\n",
    "            y=100,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            annotation_text=\"M√©dia (100)\"\n",
    "        )\n",
    "        \n",
    "        fig2.update_layout(\n",
    "            title='√çndice de Sazonalidade Mensal (Base 100 = M√©dia)',\n",
    "            xaxis_title='M√™s',\n",
    "            yaxis_title='√çndice',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        fig2.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ An√°lise de s√©ries temporais conclu√≠da!\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dados insuficientes para an√°lise temporal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc46264-344a-4c1e-9cf2-8b7633a61c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7c97d-a155-47c4-b0b0-2ac80e32f531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3b0b9-c793-4ed0-8e51-c199df691c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
